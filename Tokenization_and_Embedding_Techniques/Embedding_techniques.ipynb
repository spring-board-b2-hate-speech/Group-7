{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cebd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01264f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file = r\"C:\\Users\\prits\\Downloads\\Data\\cleaned_ghc_train.csv\"\n",
    "test_file = r'C:\\Users\\prits\\Downloads\\Data\\cleaned_ghc_test.csv'\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67302ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo label\n",
      "0      he most likely converted to islam due to his n...   0   0   0     h\n",
      "1      so ford lied about being a psychologist record...   0   0   0     h\n",
      "2      jobs education ending abuse of nation californ...   0   0   0    nh\n",
      "3      i share a lot of your values  like many who do...   0   0   0     h\n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0    nh\n",
      "...                                                  ...  ..  ..  ..   ...\n",
      "21771  im a fan of western civilization and one bedro...   0   0   0     h\n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0     h\n",
      "21773  thank you to all my followers that follow me e...   0   0   0     h\n",
      "21774                                   wednesday music    0   0   0    nh\n",
      "21775                    this is a really big surprise     0   0   0     h\n",
      "\n",
      "[21776 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c410af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prits\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\prits\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178ed4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo label  \\\n",
      "0      he most likely converted to islam due to his n...   0   0   0     h   \n",
      "1      so ford lied about being a psychologist record...   0   0   0     h   \n",
      "2      jobs education ending abuse of nation californ...   0   0   0    nh   \n",
      "3      i share a lot of your values  like many who do...   0   0   0     h   \n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0    nh   \n",
      "...                                                  ...  ..  ..  ..   ...   \n",
      "21771  im a fan of western civilization and one bedro...   0   0   0     h   \n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0     h   \n",
      "21773  thank you to all my followers that follow me e...   0   0   0     h   \n",
      "21774                                   wednesday music    0   0   0    nh   \n",
      "21775                    this is a really big surprise     0   0   0     h   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [he, most, likely, converted, to, islam, due, ...  \n",
      "1      [so, ford, lied, about, being, a, psychologist...  \n",
      "2      [jobs, education, ending, abuse, of, nation, c...  \n",
      "3      [i, share, a, lot, of, your, values, like, man...  \n",
      "4      [i, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "21771  [im, a, fan, of, western, civilization, and, o...  \n",
      "21772  [or, is, she, saying, that, muslims, dont, kno...  \n",
      "21773  [thank, you, to, all, my, followers, that, fol...  \n",
      "21774                                 [wednesday, music]  \n",
      "21775               [this, is, a, really, big, surprise]  \n",
      "\n",
      "[21776 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#word tokenisation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "# Download the necessary NLTK data files (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize each text in the DataFrame\n",
    "train_df['tokens'] = train_df['text'].apply(word_tokenize)\n",
    "\n",
    "# Print the DataFrame with tokens\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089a07a",
   "metadata": {},
   "source": [
    " TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f74a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4785)\t0.1620296940882516\n",
      "  (0, 2905)\t0.38923917472124503\n",
      "  (0, 1332)\t0.38923917472124503\n",
      "  (0, 2343)\t0.3104641687725634\n",
      "  (0, 1773)\t0.12407978854649923\n",
      "  (0, 492)\t0.21140284704973647\n",
      "  (0, 2951)\t0.32600851987838764\n",
      "  (0, 2126)\t0.18885132253462167\n",
      "  (0, 1400)\t0.2977658298377288\n",
      "  (0, 2342)\t0.27002478385343953\n",
      "  (0, 4503)\t0.18297448250041176\n",
      "  (0, 2606)\t0.30605276712057433\n",
      "  (0, 2879)\t0.22706098159854585\n",
      "  (0, 2065)\t0.17896439019209592\n",
      "  (1, 4902)\t0.26136219694263846\n",
      "  (1, 2996)\t0.19093692873719764\n",
      "  (1, 4246)\t0.3532425513174643\n",
      "  (1, 2420)\t0.18222373574691075\n",
      "  (1, 3958)\t0.23702459173372586\n",
      "  (1, 3892)\t0.3286760103770535\n",
      "  (1, 3598)\t0.37005656618250765\n",
      "  (1, 89)\t0.1845526087907791\n",
      "  (1, 2596)\t0.38347050536924204\n",
      "  (1, 1779)\t0.35822709339486697\n",
      "  (1, 4078)\t0.18314192526077977\n",
      "  :\t:\n",
      "  (21769, 3128)\t0.23006431738505032\n",
      "  (21769, 2919)\t0.14194981143002833\n",
      "  (21769, 2353)\t0.07483758427314527\n",
      "  (21769, 462)\t0.2521107373031192\n",
      "  (21769, 2960)\t0.12316556882602916\n",
      "  (21769, 683)\t0.0930012024734965\n",
      "  (21769, 4420)\t0.14569375125460238\n",
      "  (21769, 2763)\t0.21199366869629266\n",
      "  (21769, 4881)\t0.08248257731131536\n",
      "  (21769, 200)\t0.18517472049092235\n",
      "  (21769, 2340)\t0.06250619864000412\n",
      "  (21769, 223)\t0.14125715093947327\n",
      "  (21769, 1353)\t0.1069533183963426\n",
      "  (21769, 2604)\t0.0987678428596147\n",
      "  (21769, 3953)\t0.1675858319023834\n",
      "  (21769, 3054)\t0.059184946879156514\n",
      "  (21769, 4078)\t0.09871013358878872\n",
      "  (21769, 4503)\t0.1602875584674718\n",
      "  (21770, 4811)\t0.7761924236019886\n",
      "  (21770, 2916)\t0.630496091615857\n",
      "  (21771, 4309)\t0.6697170126868022\n",
      "  (21771, 524)\t0.4916098432207747\n",
      "  (21771, 4451)\t0.2657119695127845\n",
      "  (21771, 3581)\t0.440015472866383\n",
      "  (21771, 2340)\t0.21350039311936056\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def tfidf_embedding(text):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(text)\n",
    "    return embeddings\n",
    "\n",
    "text = train_df['text'].values\n",
    "embeddings_tfidf = tfidf_embedding(text)\n",
    "print(embeddings_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee154a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9116\n",
      "Precision: 0.9001\n",
      "Recall: 0.9116\n",
      "F1 Score: 0.8750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#Step 1: TF-IDF Encoding\n",
    "def tfidf_embedding(text):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(text)\n",
    "    return embeddings\n",
    "\n",
    "text = train_df['text'].values\n",
    "hd = train_df['hd'].values\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(text)\n",
    "\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_tfidf, hd, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predictions and Evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e98f9",
   "metadata": {},
   "source": [
    "Word2Vec is an effort to map words to high-dimensional vectors to capture the semantic relationships between words.Words with similar meanings should have similar vector representations, according to the main principle of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373e15ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12863833  0.18229553  0.03983613 ... -0.32184178 -0.06998452\n",
      "  -0.29096565]\n",
      " [ 0.01588773  0.26588994  0.19004579 ... -0.25425884 -0.08934192\n",
      "  -0.5033654 ]\n",
      " [-0.04830959  0.24815992 -0.02696376 ... -0.10394708  0.07483985\n",
      "  -0.07230787]\n",
      " ...\n",
      " [-0.09826891  0.53646976  0.35280833 ... -0.28204152 -0.09486767\n",
      "  -0.17183422]\n",
      " [-0.00709109  0.30483574 -0.04895918 ... -0.27518755  0.08552052\n",
      "  -0.19927996]\n",
      " [-0.10682079  0.2881086   0.05734314 ... -0.43156347 -0.1361157\n",
      "  -0.6704223 ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Extract texts from the training dataframe\n",
    "texts_train = train_df['text'].values\n",
    "\n",
    "# Handle any NaN values in the text column\n",
    "texts_train = np.where(pd.isnull(texts_train), '', texts_train)\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_train = [word_tokenize(text) for text in texts_train]\n",
    "\n",
    "# Train a Word2Vec model on the training texts\n",
    "w2v_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to create average Word2Vec embeddings for each text\n",
    "def get_average_word2vec(tokens_list, model, vocabulary, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "    for token in tokens_list:\n",
    "        if token in vocabulary:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[token])\n",
    "    if n_words > 0:\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "# Create embeddings for the training texts\n",
    "vocabulary = set(w2v_model.wv.index_to_key)\n",
    "embeddings_train = np.array([get_average_word2vec(tokens, w2v_model, vocabulary, 100) for tokens in tokenized_train])\n",
    "\n",
    "print(embeddings_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85c307",
   "metadata": {},
   "source": [
    "OneHotEncoding:One hot encoding is a technique that we use to represent categorical variables as numerical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2613f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  hd  cv  vo label  \\\n",
      "0  he most likely converted to islam due to his n...   0   0   0     h   \n",
      "1  so ford lied about being a psychologist record...   0   0   0     h   \n",
      "\n",
      "                                              tokens  \n",
      "0  [he, most, likely, converted, to, islam, due, ...  \n",
      "1  [so, ford, lied, about, being, a, psychologist...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21776 entries, 0 to 21775\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    21776 non-null  object\n",
      " 1   hd      21776 non-null  int64 \n",
      " 2   cv      21776 non-null  int64 \n",
      " 3   vo      21776 non-null  int64 \n",
      " 4   label   21776 non-null  object\n",
      " 5   tokens  21776 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 1020.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "print(train_df.head(2), sep = \" \")\n",
    "train_df.info()\n",
    "train_df['text'] = train_df['label'].astype(str)\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(train_df[['label']])\n",
    "# Converting the encoded data into an array\n",
    "encoded_array = encoded_data.toarray()\n",
    "\n",
    "# Print the encoded data (this will show the transformed categorical data)\n",
    "encoded_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0fb54",
   "metadata": {},
   "source": [
    "Label Encoding :\n",
    "Label Encoding is a technique that is used to convert categorical columns into numerical ones so that they can be fitted by machine learning models which only take numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b96363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prits\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the categorical features\n",
    "encoded_data = label_encoder.fit_transform(train_df[['label']])\n",
    "\n",
    "print(encoded_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
