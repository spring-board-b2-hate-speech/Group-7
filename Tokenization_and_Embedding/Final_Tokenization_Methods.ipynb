{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b5e59a-f3ae-4ad4-a5b8-2338c7053aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Load the TSV files\n",
    "train_file_path = r'C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.tsv'\n",
    "test_file_path = r'C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.tsv'\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "test_data = pd.read_csv(test_file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304f288-4063-4587-9278-aee2929fc361",
   "metadata": {},
   "source": [
    "TOKENIZATION (Using NLTK (Natural Language Toolkit)). NLTK is a popular library for text processing and provides various tokenizers for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2548e286-053d-460a-9950-cf0f1e120982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo  \\\n",
      "0      He most likely converted to islam due to his n...   0   0   0   \n",
      "1      So Ford lied about being a psychologist. Recor...   0   0   0   \n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   0   0   0   \n",
      "3      I share a lot of your values, & like many who ...   0   0   0   \n",
      "4      I am so ready to get back to blogging! www.ben...   0   0   0   \n",
      "...                                                  ...  ..  ..  ..   \n",
      "22031  I'm a fan of western civilization, and one bed...   0   0   0   \n",
      "22032  Or ... is she saying that Muslims don't know h...   0   0   0   \n",
      "22033  Thank you to all my followers that follow me e...   0   0   0   \n",
      "22034  Wednesday music. https://www.youtube.com/watch...   0   0   0   \n",
      "22035  This is a really Big Surprise!  https://www.wn...   0   0   0   \n",
      "\n",
      "                                          tokenized_text  \n",
      "0      [He, most, likely, converted, to, islam, due, ...  \n",
      "1      [So, Ford, lied, about, being, a, psychologist...  \n",
      "2      [Jobs, ., Education, ., Ending, abuse, of, Nat...  \n",
      "3      [I, share, a, lot, of, your, values, ,, &, lik...  \n",
      "4      [I, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "22031  [I, 'm, a, fan, of, western, civilization, ,, ...  \n",
      "22032  [Or, ..., is, she, saying, that, Muslims, do, ...  \n",
      "22033  [Thank, you, to, all, my, followers, that, fol...  \n",
      "22034  [Wednesday, music, ., https, :, //www.youtube....  \n",
      "22035  [This, is, a, really, Big, Surprise, !, https,...  \n",
      "\n",
      "[22036 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example DataFrame\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# Tokenize a specific column using NLTK\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['tokenized_text'] = df['text'].apply(tokenize_text)\n",
    "\n",
    "# Print the DataFrame with tokenized text\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2bbf8a-45ea-427d-bf42-231645abfdfd",
   "metadata": {},
   "source": [
    "Using Regular Expressions (Regex) Regex can be used for more customized tokenization based on specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4baa6b-ec89-4f56-861c-40a15ef3d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo  \\\n",
      "0      He most likely converted to islam due to his n...   0   0   0   \n",
      "1      So Ford lied about being a psychologist. Recor...   0   0   0   \n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   0   0   0   \n",
      "3      I share a lot of your values, & like many who ...   0   0   0   \n",
      "4      I am so ready to get back to blogging! www.ben...   0   0   0   \n",
      "...                                                  ...  ..  ..  ..   \n",
      "22031  I'm a fan of western civilization, and one bed...   0   0   0   \n",
      "22032  Or ... is she saying that Muslims don't know h...   0   0   0   \n",
      "22033  Thank you to all my followers that follow me e...   0   0   0   \n",
      "22034  Wednesday music. https://www.youtube.com/watch...   0   0   0   \n",
      "22035  This is a really Big Surprise!  https://www.wn...   0   0   0   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [He, most, likely, converted, to, islam, due, ...  \n",
      "1      [So, Ford, lied, about, being, a, psychologist...  \n",
      "2      [Jobs, Education, Ending, abuse, of, Nation, C...  \n",
      "3      [I, share, a, lot, of, your, values, like, man...  \n",
      "4      [I, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "22031  [I, m, a, fan, of, western, civilization, and,...  \n",
      "22032  [Or, is, she, saying, that, Muslims, don, t, k...  \n",
      "22033  [Thank, you, to, all, my, followers, that, fol...  \n",
      "22034  [Wednesday, music, https, www, youtube, com, w...  \n",
      "22035  [This, is, a, really, Big, Surprise, https, ww...  \n",
      "\n",
      "[22036 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Example DataFrame\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "df = pd.DataFrame(train_data)\n",
    "# Tokenization using regular expressions\n",
    "df['tokens'] = df['text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
    "\n",
    "# Print the DataFrame with tokens\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d6174-e436-434d-ae7f-6a460e1cbfaf",
   "metadata": {},
   "source": [
    "CHARACTER ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0abfabc-5040-4e93-a6c6-a1d88a381522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['H', 'e', ' ', 'm', 'o', 's', 't', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 'c', 'o', 'n', 'v', 'e', 'r', 't', 'e', 'd', ' ', 't', 'o', ' ', 'i', 's', 'l', 'a', 'm', ' ', 'd', 'u', 'e', ' ', 't', 'o', ' ', 'h', 'i', 's', ' ', 'n', 'a', 't', 'u', 'r', 'e', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 's', 'u', 'i', 't', 'a', 'b', 'l', 'e', ' ', 'f', 'o', 'r', ' ', ' ', 'i', 's', 'l', 'a', 'm', 'i', 'c', ' ', 'd', 'o', 'c', 't', 'r', 'i', 'n', 'e', '.', ' ', '\"', 'P', 'r', 'o', 'p', 'h', 'e', 't', '\"', ' ', 'M', 'u', 'h', 'a', 'm', 'm', 'a', 'd', ' ', 'w', 'a', 's', ' ', 'a', ' ', 'p', 's', 'y', 'c', 'h', 'o', 'p', 'a', 't', 'h', '.'], ['S', 'o', ' ', 'F', 'o', 'r', 'd', ' ', 'l', 'i', 'e', 'd', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 'a', ' ', 'p', 's', 'y', 'c', 'h', 'o', 'l', 'o', 'g', 'i', 's', 't', '.', ' ', 'R', 'e', 'c', 'o', 'r', 'd', 's', ' ', 's', 'e', 'e', 'm', ' ', 't', 'o', ' ', 'i', 'n', 'd', 'i', 'c', 'a', 't', 'e', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'j', 'u', 's', 't', ' ', 'a', ' ', 's', 't', 'u', 'd', 'e', 'n', 't', ',', ' ', 'n', 'o', ' ', 'w', 'o', 'r', 'k', '.'], ['J', 'o', 'b', 's', '.', ' ', 'E', 'd', 'u', 'c', 'a', 't', 'i', 'o', 'n', '.', ' ', 'E', 'n', 'd', 'i', 'n', 'g', ' ', 'a', 'b', 'u', 's', 'e', ' ', 'o', 'f', ' ', 'N', 'a', 't', 'i', 'o', 'n', '.', ' ', 'C', 'A', '4', '3', '.'], ['I', ' ', 's', 'h', 'a', 'r', 'e', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'a', 'l', 'u', 'e', 's', ',', ' ', '&', ' ', 'l', 'i', 'k', 'e', ' ', 'm', 'a', 'n', 'y', ' ', 'w', 'h', 'o', ' ', 'd', 'o', ',', ' ', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'c', 'a', 'l', 'l', ' ', 'm', 'y', 's', 'e', 'l', 'f', ' ', 'a', 'l', 't', ' ', 'r', 'i', 'g', 'h', 't', ';', ' ', 'I', \"'\", 'm', ' ', 'a', ' ', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'i', 's', 't', ',', ' ', '&', ' ', 'n', 'o', 't', ' ', 'c', 'i', 'v', 'i', 'c', '.', ' ', 'I', \"'\", 'd', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 't', 'h', 'o', 'u', 'g', 'h', 't', ' ', \"'\", 'a', 'l', 't', ' ', 'r', 'i', 'g', 'h', 't', \"'\", ' ', 'i', 's', ' ', 'a', 'n', ' ', 'u', 'm', 'b', 'r', 'e', 'l', 'l', 'a', ' ', 't', 'e', 'r', 'm', ' ', 't', 'h', 'o', ',', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'm', 'a', 'n', 'y', ' ', 'a', 'r', 'e', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'a', 'l', 't', ' ', 'l', 'i', 't', 'e', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 't', 's', 'e', 'l', 'f', ' ', 'i', 's', ' ', 'l', 'a', 'r', 'g', 'e', 'l', 'y', ' ', 'c', 'l', 'a', 's', 's', 'i', 'c', 'a', 'l', ' ', 'l', 'i', 'b', 'e', 'r', 'a', 'l', ' ', 'o', 'r', ' ', 'c', 'i', 'v', 'i', 'c', ' ', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'i', 's', 't', '.', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', \"'\", 'i', 'n', 'f', 'i', 'l', 't', 'r', 'a', 't', 'o', 'r', 's', \"'\", ' ', 't', 'r', 'y', 'i', 'n', 'g', ' ', 't', 'o', ' ', 's', 'o', 'w', ' ', 'd', 'i', 's', 'c', 'o', 'r', 'd', ',', ' ', 'w', '/', ' ', 's', 'o', 'm', 'e', ' ', 's', 'u', 'c', 'c', 'e', 's', 's', '.', ' ', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'l', 'i', 'k', 'e', ' ', 'i', 'n', 'f', 'i', 'g', 'h', 't', 'i', 'n', 'g', '.'], ['I', ' ', 'a', 'm', ' ', 's', 'o', ' ', 'r', 'e', 'a', 'd', 'y', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 'b', 'a', 'c', 'k', ' ', 't', 'o', ' ', 'b', 'l', 'o', 'g', 'g', 'i', 'n', 'g', '!', ' ', 'w', 'w', 'w', '.', 'b', 'e', 'n', 'b', 'r', 'i', 'h', 'o', 'u', 's', 'e', '.', 'c', 'o', 'm', ' ', '#', 'r', 'e', 'c', 'i', 'p', 'e', 's', ' ', '#', 'f', 'o', 'o', 'd', 'p', 'h', 'o', 't', 'o', 'g', 'r', 'a', 'p', 'h', 'y'], ['t', 'a', 'k', 'i', 'n', 'g', ' ', 'a', ' ', 'l', 'o', 'o', 'k', ' ', 'a', 't', ' ', 'n', 'e', 'w', ' ', 'o', 'p', 'p', 'o', 'r', 't', 'u', 'n', 'i', 't', 'y', ' ', 'c', 'a', 'l', 'l', 'e', 'd', ' ', 'F', 'X', ' ', 'P', 'r', 'o', 'g', '.', ' ', 'I', 't', \"'\", 's', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 'o', 'p', 'p', 'o', 'r', 't', 'u', 'n', 'i', 't', 'y', ' ', 't', 'h', 'o', 'u', 'g', 'h', ' ', 'a', 'n', 'd', ' ', 't', 'o', 'd', 'a', 'y', ' ', 'I', \"'\", 'm', ' ', 'a', 's', 'k', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'n', 'c', 'e', 'r', 'n', ' ', 'i', 's', ' ', 'F', 'X', ' ', 'P', 'r', 'o', 'g', ' ', 'l', 'e', 'g', 'i', 't', ' ', 'o', 'r', ' ', 'j', 'u', 's', 't', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'r', 'i', 'p', '-', 'o', 'f', 'f', '?', ' ', 'L', 'e', 't', \"'\", 's', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'l', 'o', 'o', 'k', ' ', 'a', 'n', 'd', ' ', 'd', 'i', 's', 'c', 'o', 'v', 'e', 'r', '.', ':', ' ', 'h', 't', 't', 'p', 's', ':', '/', '/', 'a', 'a', 'r', 'o', 'n', 's', 'h', 'a', 'r', 'a', '.', 'c', 'o', 'm', '/', 'f', 'x', '-', 'p', 'r', 'o', 'g', '-', 'r', 'e', 'v', 'i', 'e', 'w', '.', 'h', 't', 'm', 'l', ' ', '#', 'F', 'X', '_', 'P', 'r', 'o', 'g', ' ', '#', 'f', 'x', '_', 'p', 'r', 'o', 'g', '_', 'r', 'e', 'v', 'i', 'e', 'w', 's', ' ', '#', 'F', 'X', '_', 'P', 'r', 'o', 'g', '_', 'r', 'e', 'v', 'i', 'e', 'w', ' ', '#', 'F', 'X', '_', 'P', 'r', 'o', 'g', '_', 's', 'c', 'a', 'm', ' ', '#', 'f', 'x', 'p', 'r', 'o', 'g'], ['R', 'e', 'f', 'l', 'e', 'c', 't', 'i', 'n', 'g', ' ', 'b', 'a', 'c', 'k', ' ', 'w', 'h', 'e', 'n', ' ', 'I', ' ', 'w', 'a', 's', ' ', 'i', 'n', ' ', 's', 'c', 'h', 'o', 'o', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'S', 'p', 'e', 'c', 'i', 'a', 'l', ' ', 'E', 'd', '.', ' ', 'I', ' ', 'w', 'a', 's', ' ', 'w', 'i', 't', 'h', ' ', 's', 't', 'u', 'd', 'e', 'n', 't', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'm', 'e', ' ', 't', 'h', 'a', 't', ' ', 'a', 'r', 'e', ' ', 'h', 'i', 'g', 'h', ' ', 'f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 'i', 'n', 'g', ' ', 'b', 'u', 't', ' ', 'n', 'e', 'e', 'd', ' ', 'e', 'x', 't', 'r', 'a', ' ', 'h', 'e', 'l', 'p', ' ', 'a', 't', ' ', 't', 'i', 'm', 'e', 's', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 'A', '.', 'D', '.', 'D', '.', ' ', 'o', 'r', ' ', 'p', 'l', 'a', 'i', 'n', ' ', 'h', 'y', 'p', 'e', 'r', ' ', 'a', 'c', 't', 'i', 'v', 'e', ' ', 't', 'h', 'a', 't', ' ', 'I', ' ', 'w', 'a', 's', ' ', 'a', ' ', 'h', 'y', 'p', 'e', 'r', ' ', 'c', 'h', 'i', 'l', 'd', '.', ' ']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "texts = train_data['text'].values\n",
    "\n",
    "# Define a function to perform character tokenization\n",
    "def character_tokenization(texts):\n",
    "    tokenized_texts = [[char for char in text] for text in texts]\n",
    "    return tokenized_texts\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_texts = character_tokenization(texts)\n",
    "print(tokenized_texts[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80671a27-6f1f-4cb4-8eea-c829b20a9ead",
   "metadata": {},
   "source": [
    "WHITESPACE TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a96e1a-422a-4743-9a00-9c4a3591c869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hd</th>\n",
       "      <th>cv</th>\n",
       "      <th>vo</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He most likely converted to islam due to his n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[He, most, likely, converted, to, islam, due, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So Ford lied about being a psychologist. Recor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[So, Ford, lied, about, being, a, psychologist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jobs. Education. Ending abuse of Nation. CA43.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Jobs., Education., Ending, abuse, of, Nation....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I share a lot of your values, &amp; like many who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, share, a, lot, of, your, values,, &amp;, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am so ready to get back to blogging! www.ben...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, am, so, ready, to, get, back, to, blogging...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hd  cv  vo  \\\n",
       "0  He most likely converted to islam due to his n...   0   0   0   \n",
       "1  So Ford lied about being a psychologist. Recor...   0   0   0   \n",
       "2     Jobs. Education. Ending abuse of Nation. CA43.   0   0   0   \n",
       "3  I share a lot of your values, & like many who ...   0   0   0   \n",
       "4  I am so ready to get back to blogging! www.ben...   0   0   0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [He, most, likely, converted, to, islam, due, ...  \n",
       "1  [So, Ford, lied, about, being, a, psychologist...  \n",
       "2  [Jobs., Education., Ending, abuse, of, Nation....  \n",
       "3  [I, share, a, lot, of, your, values,, &, like,...  \n",
       "4  [I, am, so, ready, to, get, back, to, blogging...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#whitespacetokenisation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Initialize the whitespace tokenizer\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "def  whitespace_tokenize(text):\n",
    "    return whitespace_tokenizer.tokenize(text)\n",
    "train_data['tokens'] = train_data['text'].apply(whitespace_tokenize)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be2769-4ed7-4581-8d9a-662ef99a948c",
   "metadata": {},
   "source": [
    "PUNCTUATION TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff157ec-ab13-4720-a76b-23f3f5081e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BHARGAVI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0      He most likely converted to islam due to his n...   \n",
      "1      So Ford lied about being a psychologist. Recor...   \n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   \n",
      "3      I share a lot of your values, & like many who ...   \n",
      "4      I am so ready to get back to blogging! www.ben...   \n",
      "...                                                  ...   \n",
      "22031  I'm a fan of western civilization, and one bed...   \n",
      "22032  Or ... is she saying that Muslims don't know h...   \n",
      "22033  Thank you to all my followers that follow me e...   \n",
      "22034  Wednesday music. https://www.youtube.com/watch...   \n",
      "22035  This is a really Big Surprise!  https://www.wn...   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [He, most, likely, converted, to, islam, due, ...  \n",
      "1      [So, Ford, lied, about, being, a, psychologist...  \n",
      "2      [Jobs, ., Education, ., Ending, abuse, of, Nat...  \n",
      "3      [I, share, a, lot, of, your, values, ,, &, lik...  \n",
      "4      [I, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "22031  [I, ', m, a, fan, of, western, civilization, ,...  \n",
      "22032  [Or, ..., is, she, saying, that, Muslims, don,...  \n",
      "22033  [Thank, you, to, all, my, followers, that, fol...  \n",
      "22034  [Wednesday, music, ., https, ://, www, ., yout...  \n",
      "22035  [This, is, a, really, Big, Surprise, !, https,...  \n",
      "\n",
      "[22036 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Punctuation tokenization, also known as punctuation-aware tokenization, is a method of splitting text into tokens while considering punctuation marks. Unlike simple whitespace tokenization, punctuation tokenization ensures that punctuation marks are treated as separate tokens rather than being attached to words\n",
    "\n",
    "#punctutaion tokenisation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# Initialize the RegexpTokenizer for punctuation tokenization\n",
    "punctuation_tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
    "\n",
    "# Define a function for punctuation tokenization\n",
    "def punctuation_tokenize(text):\n",
    "    return punctuation_tokenizer.tokenize(text)\n",
    "\n",
    "# Apply the punctuation_tokenize function to the 'text' column\n",
    "train_data['tokens'] = train_data['text'].apply(punctuation_tokenize)\n",
    "\n",
    "# Display the text and tokens columns side by side\n",
    "result_data = train_data[['text', 'tokens']]\n",
    "\n",
    "print(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fde5d7-7229-46e1-9c06-2f41dacc8a80",
   "metadata": {},
   "source": [
    "SUBWORD TOKENISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e5baca-12e6-404c-921f-4ab43cbf4a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tokenizers) (0.23.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bhargavi\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhargavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24d8c851-cb2a-45d1-a491-8b3f7f8d909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hd</th>\n",
       "      <th>cv</th>\n",
       "      <th>vo</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He most likely converted to islam due to his n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[He, most, likely, con, ver, ted, to, islam, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So Ford lied about being a psychologist. Recor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[So, Ford, lied, about, being, a, psych, olog,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jobs. Education. Ending abuse of Nation. CA43.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[J, ob, s, ., E, du, cation, ., En, ding, abus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I share a lot of your values, &amp; like many who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, share, a, lot, of, your, values, ,, &amp;, lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am so ready to get back to blogging! www.ben...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, am, so, ready, to, get, back, to, blo, gg,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hd  cv  vo  \\\n",
       "0  He most likely converted to islam due to his n...   0   0   0   \n",
       "1  So Ford lied about being a psychologist. Recor...   0   0   0   \n",
       "2     Jobs. Education. Ending abuse of Nation. CA43.   0   0   0   \n",
       "3  I share a lot of your values, & like many who ...   0   0   0   \n",
       "4  I am so ready to get back to blogging! www.ben...   0   0   0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [He, most, likely, con, ver, ted, to, islam, d...  \n",
       "1  [So, Ford, lied, about, being, a, psych, olog,...  \n",
       "2  [J, ob, s, ., E, du, cation, ., En, ding, abus...  \n",
       "3  [I, share, a, lot, of, your, values, ,, &, lik...  \n",
       "4  [I, am, so, ready, to, get, back, to, blo, gg,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subword tokenization is a process where words are broken down into smaller units, which can help in handling out-of-vocabulary words and capturing subword-level information\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "subwords = train_data['text'].dropna().tolist()\n",
    "# Initialize the tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Pre-tokenization with whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "# Set up a trainer\n",
    "trainer = BpeTrainer(vocab_size=5000, min_frequency=2, special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[MASK]\"])\n",
    "# Train the tokenizer on the subwords\n",
    "tokenizer.train_from_iterator(subwords, trainer)\n",
    "# Tokenize the text data\n",
    "train_data['tokens'] = train_data['text'].dropna().apply(lambda x: tokenizer.encode(x).tokens)\n",
    "\n",
    "# Display the tokenized dataframe\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a8a1f-5cb0-4d8e-b4dd-41d363b44a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
