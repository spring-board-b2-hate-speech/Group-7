{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae59e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e5e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file = r\"C:\\Users\\prits\\Downloads\\Data\\cleaned_ghc_train.csv\"\n",
    "test_file = r'C:\\Users\\prits\\Downloads\\Data\\cleaned_ghc_test.csv'\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4348e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo label\n",
      "0      he most likely converted to islam due to his n...   0   0   0     h\n",
      "1      so ford lied about being a psychologist record...   0   0   0     h\n",
      "2      jobs education ending abuse of nation californ...   0   0   0    nh\n",
      "3      i share a lot of your values  like many who do...   0   0   0     h\n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0    nh\n",
      "...                                                  ...  ..  ..  ..   ...\n",
      "21771  im a fan of western civilization and one bedro...   0   0   0     h\n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0     h\n",
      "21773  thank you to all my followers that follow me e...   0   0   0     h\n",
      "21774                                   wednesday music    0   0   0    nh\n",
      "21775                    this is a really big surprise     0   0   0     h\n",
      "\n",
      "[21776 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0207dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prits\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prits\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37d1e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo label  \\\n",
      "0      he most likely converted to islam due to his n...   0   0   0     h   \n",
      "1      so ford lied about being a psychologist record...   0   0   0     h   \n",
      "2      jobs education ending abuse of nation californ...   0   0   0    nh   \n",
      "3      i share a lot of your values  like many who do...   0   0   0     h   \n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0    nh   \n",
      "...                                                  ...  ..  ..  ..   ...   \n",
      "21771  im a fan of western civilization and one bedro...   0   0   0     h   \n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0     h   \n",
      "21773  thank you to all my followers that follow me e...   0   0   0     h   \n",
      "21774                                   wednesday music    0   0   0    nh   \n",
      "21775                    this is a really big surprise     0   0   0     h   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [he, most, likely, converted, to, islam, due, ...  \n",
      "1      [so, ford, lied, about, being, a, psychologist...  \n",
      "2      [jobs, education, ending, abuse, of, nation, c...  \n",
      "3      [i, share, a, lot, of, your, values, like, man...  \n",
      "4      [i, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "21771  [im, a, fan, of, western, civilization, and, o...  \n",
      "21772  [or, is, she, saying, that, muslims, dont, kno...  \n",
      "21773  [thank, you, to, all, my, followers, that, fol...  \n",
      "21774                                 [wednesday, music]  \n",
      "21775               [this, is, a, really, big, surprise]  \n",
      "\n",
      "[21776 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#word tokenisation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "# Download the necessary NLTK data files (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize each text in the DataFrame\n",
    "train_df['tokens'] = train_df['text'].apply(word_tokenize)\n",
    "\n",
    "# Print the DataFrame with tokens\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c41679",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14df6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 24880)\t0.31710913639027155\n",
      "  (0, 33748)\t0.12467565088586496\n",
      "  (0, 20751)\t0.29949577210948686\n",
      "  (0, 24714)\t0.31179399174676026\n",
      "  (0, 9590)\t0.29949577210948686\n",
      "  (0, 16715)\t0.2388844980949804\n",
      "  (0, 12447)\t0.09547613390817968\n",
      "  (0, 30263)\t0.3310896501311571\n",
      "  (0, 3841)\t0.1626644731914859\n",
      "  (0, 21095)\t0.2508446739516041\n",
      "  (0, 14985)\t0.14531281938493537\n",
      "  (0, 10012)\t0.22911410847152885\n",
      "  (0, 16704)\t0.20776951929183204\n",
      "  (0, 31641)\t0.14079737944241671\n",
      "  (0, 7356)\t0.31710913639027155\n",
      "  (0, 18500)\t0.23549026573540505\n",
      "  (0, 20598)\t0.17471219660660267\n",
      "  (0, 14575)\t0.13770558967852184\n",
      "  (1, 34475)\t0.2195790341458713\n",
      "  (1, 21519)\t0.16041445402598734\n",
      "  (1, 30069)\t0.2967681272991259\n",
      "  (1, 17319)\t0.1530944619562374\n",
      "  (1, 28127)\t0.1991329040848775\n",
      "  (1, 16045)\t0.375163973280694\n",
      "  (1, 27694)\t0.2761296673148862\n",
      "  :\t:\n",
      "  (21773, 22552)\t0.21671870343794195\n",
      "  (21773, 20868)\t0.13371450055069928\n",
      "  (21773, 16784)\t0.07049797403608575\n",
      "  (21773, 3706)\t0.23748991109226655\n",
      "  (21773, 21176)\t0.11602063728705314\n",
      "  (21773, 5133)\t0.08760723862478234\n",
      "  (21773, 31151)\t0.13724564231645003\n",
      "  (21773, 19630)\t0.19969701170807724\n",
      "  (21773, 34345)\t0.07769919392663961\n",
      "  (21773, 1984)\t0.17443483785180464\n",
      "  (21773, 16689)\t0.05888239517864292\n",
      "  (21773, 2107)\t0.13306204730750051\n",
      "  (21773, 9710)\t0.1007494683426215\n",
      "  (21773, 18493)\t0.09303913964063014\n",
      "  (21773, 28093)\t0.15786241273563406\n",
      "  (21773, 22090)\t0.05575393395934041\n",
      "  (21773, 28962)\t0.09298478025182932\n",
      "  (21773, 31641)\t0.1509968432053895\n",
      "  (21774, 33885)\t0.7761907953532575\n",
      "  (21774, 20849)\t0.6304980961183604\n",
      "  (21775, 30426)\t0.6697105530989748\n",
      "  (21775, 4029)\t0.4916094684422753\n",
      "  (21775, 31359)\t0.2657193122534239\n",
      "  (21775, 25540)\t0.4400168607452037\n",
      "  (21775, 16689)\t0.21350951960339454\n",
      "(21776, 35368)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "text = train_df['text'].tolist()\n",
    "label = train_df['label'].tolist()\n",
    "\n",
    "def tfidf_embedding(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(text)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(text)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(embeddings_tfidf, label, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(embeddings_tfidf)\n",
    "print(embeddings_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386be9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_evaluate_rf(X_train_emb, X_test_emb, y_train, y_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_emb, y_train)\n",
    "    y_pred = rf.predict(X_test_emb)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5560781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.720615243342516\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           h       0.78      0.19      0.30      1407\n",
      "          nh       0.72      0.97      0.83      2949\n",
      "\n",
      "    accuracy                           0.72      4356\n",
      "   macro avg       0.75      0.58      0.57      4356\n",
      "weighted avg       0.74      0.72      0.66      4356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_tfidf, report_tfidf = train_evaluate_rf(X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf)\n",
    "print(f'TF-IDF Accuracy: {accuracy_tfidf}')\n",
    "print(f'TF-IDF Classification Report:\\n{report_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7db01",
   "metadata": {},
   "source": [
    "Word2Vec is an effort to map words to high-dimensional vectors to capture the semantic relationships between words.Words with similar meanings should have similar vector representations, according to the main principle of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67893ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04996056  0.3125958  -0.10564645 ... -0.36408773 -0.11938274\n",
      "  -0.25820625]\n",
      " [-0.15601371  0.3611218  -0.04419656 ... -0.28170615 -0.12261149\n",
      "  -0.41080427]\n",
      " [-0.10266089  0.341702   -0.04805031 ... -0.19066872  0.06783475\n",
      "  -0.10286985]\n",
      " ...\n",
      " [-0.29624313  0.6246821   0.17197324 ... -0.3617536  -0.04014009\n",
      "  -0.28844056]\n",
      " [-0.04265551  0.3442541  -0.10895921 ... -0.37010425 -0.01953383\n",
      "  -0.16869847]\n",
      " [-0.43342534  0.43683624 -0.14489152 ... -0.5334764  -0.06727689\n",
      "  -0.47319642]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Extract texts from the training dataframe\n",
    "texts_train = train_df['text'].values\n",
    "\n",
    "# Handle any NaN values in the text column\n",
    "texts_train = np.where(pd.isnull(texts_train), '', texts_train)\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_train = [word_tokenize(text) for text in texts_train]\n",
    "\n",
    "# Train a Word2Vec model on the training texts\n",
    "w2v_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to create average Word2Vec embeddings for each text\n",
    "def get_average_word2vec(tokens_list, model, vocabulary, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "    for token in tokens_list:\n",
    "        if token in vocabulary:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[token])\n",
    "    if n_words > 0:\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "# Create embeddings for the training texts\n",
    "vocabulary = set(w2v_model.wv.index_to_key)\n",
    "embeddings_train = np.array([get_average_word2vec(tokens, w2v_model, vocabulary, 100) for tokens in tokenized_train])\n",
    "\n",
    "print(embeddings_train)\n",
    "# Split the embeddings and labels into training and testing sets (80% train, 20% test)\n",
    "X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2 = train_test_split(embeddings_train, label, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ea33de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.8266758494031221\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_w2v2, report_w2v2 = train_evaluate_rf(X_train_w2v2, X_test_w2v2, y_train_w2v2, y_test_w2v2)\n",
    "print(f'Word2Vec Accuracy: {accuracy_w2v2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d3800",
   "metadata": {},
   "source": [
    "OneHotEncoding:One hot encoding is a technique that we use to represent categorical variables as numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c12143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prits\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd_0  hd_1  cv_0  \\\n",
      "0      he most likely converted to islam due to his n...   1.0   0.0   1.0   \n",
      "1      so ford lied about being a psychologist record...   1.0   0.0   1.0   \n",
      "2      jobs education ending abuse of nation californ...   1.0   0.0   1.0   \n",
      "3      i share a lot of your values  like many who do...   1.0   0.0   1.0   \n",
      "4      i am so ready to get back to blogging  recipes...   1.0   0.0   1.0   \n",
      "...                                                  ...   ...   ...   ...   \n",
      "21771  im a fan of western civilization and one bedro...   1.0   0.0   1.0   \n",
      "21772  or  is she saying that muslims dont know how t...   1.0   0.0   1.0   \n",
      "21773  thank you to all my followers that follow me e...   1.0   0.0   1.0   \n",
      "21774                                   wednesday music    1.0   0.0   1.0   \n",
      "21775                    this is a really big surprise     1.0   0.0   1.0   \n",
      "\n",
      "       cv_1  vo_0  vo_1  label_h  label_nh  \n",
      "0       0.0   1.0   0.0      1.0       0.0  \n",
      "1       0.0   1.0   0.0      1.0       0.0  \n",
      "2       0.0   1.0   0.0      0.0       1.0  \n",
      "3       0.0   1.0   0.0      1.0       0.0  \n",
      "4       0.0   1.0   0.0      0.0       1.0  \n",
      "...     ...   ...   ...      ...       ...  \n",
      "21771   0.0   1.0   0.0      1.0       0.0  \n",
      "21772   0.0   1.0   0.0      1.0       0.0  \n",
      "21773   0.0   1.0   0.0      1.0       0.0  \n",
      "21774   0.0   1.0   0.0      0.0       1.0  \n",
      "21775   0.0   1.0   0.0      1.0       0.0  \n",
      "\n",
      "[21776 rows x 9 columns]\n",
      "OneHot accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = train_df\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Encode categorical columns\n",
    "encoded_data = encoder.fit_transform(df[['hd', 'cv', 'vo', 'label']])\n",
    "\n",
    "# Convert the encoded data into a DataFrame with appropriate column names\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names(['hd', 'cv', 'vo', 'label']))\n",
    "\n",
    "# Concatenate the original 'text' column with the encoded DataFrame\n",
    "final_df = pd.concat([df[['text']], encoded_df], axis=1)\n",
    "print(final_df)\n",
    "\n",
    "# Assuming 'label' column is the target variable\n",
    "X = final_df.drop(columns=['text'])  # Features\n",
    "y = final_df[['label_h', 'label_nh']]  # Target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"OneHot accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbaa76a",
   "metadata": {},
   "source": [
    "Label Encoding:Label Encoding is a technique that is used to convert categorical columns into numerical ones so that they can be fitted by machine learning models which only take numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae59a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prits\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the categorical features\n",
    "encoded_data = label_encoder.fit_transform(train_df[['label']])\n",
    "\n",
    "print(encoded_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
