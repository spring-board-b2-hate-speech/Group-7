{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5977f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914040bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSV files\n",
    "train_file_path = r'C:\\Users\\prits\\Downloads\\Data\\ghc_train.tsv'\n",
    "test_file_path = r'C:\\Users\\prits\\Downloads\\Data\\ghc_test.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_file_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9bf4047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'hd', 'cv', 'vo'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2246cf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hd</th>\n",
       "      <th>cv</th>\n",
       "      <th>vo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He most likely converted to islam due to his n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So Ford lied about being a psychologist. Recor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jobs. Education. Ending abuse of Nation. CA43.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I share a lot of your values, &amp; like many who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am so ready to get back to blogging! www.ben...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>taking a look at new opportunity called FX Pro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reflecting back when I was in school with Spec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Let's be honest everyone, last year there were...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2007 Nuke plant in Syria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NBC's Chuck Todd Thinks He's Figured It All Ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hd  cv  vo\n",
       "0  He most likely converted to islam due to his n...   0   0   0\n",
       "1  So Ford lied about being a psychologist. Recor...   0   0   0\n",
       "2     Jobs. Education. Ending abuse of Nation. CA43.   0   0   0\n",
       "3  I share a lot of your values, & like many who ...   0   0   0\n",
       "4  I am so ready to get back to blogging! www.ben...   0   0   0\n",
       "5  taking a look at new opportunity called FX Pro...   0   0   0\n",
       "6  Reflecting back when I was in school with Spec...   0   0   0\n",
       "7  Let's be honest everyone, last year there were...   0   0   0\n",
       "8                          2007 Nuke plant in Syria    0   0   0\n",
       "9  NBC's Chuck Todd Thinks He's Figured It All Ou...   0   0   0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 rows in our dataset\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4975d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding duplicates\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c2624ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the duplicates and keeing just the first occurrence\n",
    "train_df = train_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c694eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\prits\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\prits\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: click in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prits\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c834c03",
   "metadata": {},
   "source": [
    "# WHITESPACE TOKENISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f874b5",
   "metadata": {},
   "source": [
    "\n",
    "Whitespace tokenization is a simple method of breaking down text into tokens based on whitespace characters such as spaces, tabs, and newlines. This method is straightforward and often used as a preliminary step in text processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce571678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hd</th>\n",
       "      <th>cv</th>\n",
       "      <th>vo</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He most likely converted to islam due to his n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[He, most, likely, converted, to, islam, due, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So Ford lied about being a psychologist. Recor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[So, Ford, lied, about, being, a, psychologist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jobs. Education. Ending abuse of Nation. CA43.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Jobs., Education., Ending, abuse, of, Nation....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I share a lot of your values, &amp; like many who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, share, a, lot, of, your, values,, &amp;, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am so ready to get back to blogging! www.ben...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, am, so, ready, to, get, back, to, blogging...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hd  cv  vo  \\\n",
       "0  He most likely converted to islam due to his n...   0   0   0   \n",
       "1  So Ford lied about being a psychologist. Recor...   0   0   0   \n",
       "2     Jobs. Education. Ending abuse of Nation. CA43.   0   0   0   \n",
       "3  I share a lot of your values, & like many who ...   0   0   0   \n",
       "4  I am so ready to get back to blogging! www.ben...   0   0   0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [He, most, likely, converted, to, islam, due, ...  \n",
       "1  [So, Ford, lied, about, being, a, psychologist...  \n",
       "2  [Jobs., Education., Ending, abuse, of, Nation....  \n",
       "3  [I, share, a, lot, of, your, values,, &, like,...  \n",
       "4  [I, am, so, ready, to, get, back, to, blogging...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#whitespacetokenisation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Initialize the whitespace tokenizer\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "def  whitespace_tokenize(text):\n",
    "    return whitespace_tokenizer.tokenize(text)\n",
    "train_df['tokens'] = train_df['text'].apply(whitespace_tokenize)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e72dd9",
   "metadata": {},
   "source": [
    "# Punctuation Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8598bb3a",
   "metadata": {},
   "source": [
    "Punctuation tokenization, also known as punctuation-aware tokenization, is a method of splitting text into tokens while considering punctuation marks. Unlike simple whitespace tokenization, punctuation tokenization ensures that punctuation marks are treated as separate tokens rather than being attached to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7679fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0      He most likely converted to islam due to his n...   \n",
      "1      So Ford lied about being a psychologist. Recor...   \n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   \n",
      "3      I share a lot of your values, & like many who ...   \n",
      "4      I am so ready to get back to blogging! www.ben...   \n",
      "...                                                  ...   \n",
      "22031  I'm a fan of western civilization, and one bed...   \n",
      "22032  Or ... is she saying that Muslims don't know h...   \n",
      "22033  Thank you to all my followers that follow me e...   \n",
      "22034  Wednesday music. https://www.youtube.com/watch...   \n",
      "22035  This is a really Big Surprise!  https://www.wn...   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [He, most, likely, converted, to, islam, due, ...  \n",
      "1      [So, Ford, lied, about, being, a, psychologist...  \n",
      "2      [Jobs, ., Education, ., Ending, abuse, of, Nat...  \n",
      "3      [I, share, a, lot, of, your, values, ,, &, lik...  \n",
      "4      [I, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "22031  [I, ', m, a, fan, of, western, civilization, ,...  \n",
      "22032  [Or, ..., is, she, saying, that, Muslims, don,...  \n",
      "22033  [Thank, you, to, all, my, followers, that, fol...  \n",
      "22034  [Wednesday, music, ., https, ://, www, ., yout...  \n",
      "22035  [This, is, a, really, Big, Surprise, !, https,...  \n",
      "\n",
      "[21964 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#punctutaion tokenisation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# Initialize the RegexpTokenizer for punctuation tokenization\n",
    "punctuation_tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
    "\n",
    "# Define a function for punctuation tokenization\n",
    "def punctuation_tokenize(text):\n",
    "    return punctuation_tokenizer.tokenize(text)\n",
    "\n",
    "# Apply the punctuation_tokenize function to the 'text' column\n",
    "train_df['tokens'] = train_df['text'].apply(punctuation_tokenize)\n",
    "\n",
    "# Display the text and tokens columns side by side\n",
    "result_df = train_df[['text', 'tokens']]\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8cf3b",
   "metadata": {},
   "source": [
    "# SUBWORD TOKENISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5e955",
   "metadata": {},
   "source": [
    "\n",
    "Subword tokenization is a process where words are broken down into smaller units, which can help in handling out-of-vocabulary words and capturing subword-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35cbd2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'He', 'most', 'likely', 'con', 'ver', 'ted', 'to', 'islam', 'due', 'to', 'his', 'nature', 'being', 'suit', 'able', 'for', 'islam', 'ic', 'doctr', 'ine', '.', '\"', 'Pro', 'phe', 't', '\"', 'Mu', 'ham', 'mad', 'was', 'a', 'psych', 'op', 'ath', '.', '</s>'], ['<s>', 'So', 'Ford', 'lied', 'about', 'being', 'a', 'psych', 'olog', 'ist', '.', 'Re', 'cord', 's', 'seem', 'to', 'ind', 'ic', 'ate', 'she', 'was', 'just', 'a', 'student', ',', 'no', 'work', '.', '</s>'], ['<s>', 'J', 'ob', 's', '.', 'E', 'du', 'cation', '.', 'En', 'ding', 'abuse', 'of', 'Nation', '.', 'CA', '43', '.', '</s>'], ['<s>', 'I', 'share', 'a', 'lot', 'of', 'your', 'values', ',', '&', 'like', 'many', 'who', 'do', ',', 'I', 'don', \"'\", 't', 'call', 'myself', 'alt', 'right', ';', 'I', \"'\", 'm', 'a', 'national', 'ist', ',', '&', 'not', 'civ', 'ic', '.', 'I', \"'\", 'd', 'always', 'thought', \"'\", 'alt', 'right', \"'\", 'is', 'an', 'um', 'bre', 'll', 'a', 'term', 'th', 'o', ',', 'where', 'many', 'are', 'really', 'alt', 'l', 'ite', ',', 'which', 'itself', 'is', 'large', 'ly', 'class', 'ical', 'liberal', 'or', 'civ', 'ic', 'national', 'ist', '.', 'There', 'are', 'a', 'lot', 'of', \"'\", 'in', 'fil', 'tr', 'ators', \"'\", 'trying', 'to', 's', 'ow', 'dis', 'cord', ',', 'w', '/', 'some', 'success', '.', 'I', 'don', \"'\", 't', 'like', 'in', 'fighting', '.', '</s>'], ['<s>', 'I', 'am', 'so', 'ready', 'to', 'get', 'back', 'to', 'blo', 'gg', 'ing', '!', 'www', '.', 'ben', 'br', 'i', 'house', '.', 'com', '#', 'reci', 'pes', '#', 'food', 'photo', 'graphy', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "from tokenizers.processors import BertProcessing\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming train_df is already defined and loaded\n",
    "texts = train_df['text'].values\n",
    "\n",
    "# Apply the mask to both texts and labels\n",
    "texts = texts[mask]\n",
    "\n",
    "\n",
    "# Define a function to train a subword tokenizer\n",
    "def train_bpe_tokenizer(texts):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(vocab_size=5000, min_frequency=2, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "    \n",
    "    tokenizer.post_processor = BertProcessing(\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "    )\n",
    "    \n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Train the BPE tokenizer\n",
    "tokenizer = train_bpe_tokenizer(texts)\n",
    "\n",
    "# Tokenize texts using the trained tokenizer\n",
    "def tokenize_texts(texts, tokenizer):\n",
    "    tokenized_texts = [tokenizer.encode(text).tokens for text in texts]\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_texts = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "print(tokenized_texts[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00498f",
   "metadata": {},
   "source": [
    "# CHARACTER TOKENISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88fa75",
   "metadata": {},
   "source": [
    "Character tokenization is a method of breaking down text into individual characters, rather than words or subwords. This technique is particularly useful for languages with complex morphology, or in cases where handling out-of-vocabulary words is important. Character tokenization allows models to learn representations at the character level, which can capture finer-grained information and can be useful in various NLP tasks like language modeling, text generation, and spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6772d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['H', 'e', ' ', 'm', 'o', 's', 't', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 'c', 'o', 'n', 'v', 'e', 'r', 't', 'e', 'd', ' ', 't', 'o', ' ', 'i', 's', 'l', 'a', 'm', ' ', 'd', 'u', 'e', ' ', 't', 'o', ' ', 'h', 'i', 's', ' ', 'n', 'a', 't', 'u', 'r', 'e', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 's', 'u', 'i', 't', 'a', 'b', 'l', 'e', ' ', 'f', 'o', 'r', ' ', ' ', 'i', 's', 'l', 'a', 'm', 'i', 'c', ' ', 'd', 'o', 'c', 't', 'r', 'i', 'n', 'e', '.', ' ', '\"', 'P', 'r', 'o', 'p', 'h', 'e', 't', '\"', ' ', 'M', 'u', 'h', 'a', 'm', 'm', 'a', 'd', ' ', 'w', 'a', 's', ' ', 'a', ' ', 'p', 's', 'y', 'c', 'h', 'o', 'p', 'a', 't', 'h', '.'], ['S', 'o', ' ', 'F', 'o', 'r', 'd', ' ', 'l', 'i', 'e', 'd', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 'a', ' ', 'p', 's', 'y', 'c', 'h', 'o', 'l', 'o', 'g', 'i', 's', 't', '.', ' ', 'R', 'e', 'c', 'o', 'r', 'd', 's', ' ', 's', 'e', 'e', 'm', ' ', 't', 'o', ' ', 'i', 'n', 'd', 'i', 'c', 'a', 't', 'e', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'j', 'u', 's', 't', ' ', 'a', ' ', 's', 't', 'u', 'd', 'e', 'n', 't', ',', ' ', 'n', 'o', ' ', 'w', 'o', 'r', 'k', '.'], ['J', 'o', 'b', 's', '.', ' ', 'E', 'd', 'u', 'c', 'a', 't', 'i', 'o', 'n', '.', ' ', 'E', 'n', 'd', 'i', 'n', 'g', ' ', 'a', 'b', 'u', 's', 'e', ' ', 'o', 'f', ' ', 'N', 'a', 't', 'i', 'o', 'n', '.', ' ', 'C', 'A', '4', '3', '.'], ['I', ' ', 's', 'h', 'a', 'r', 'e', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'a', 'l', 'u', 'e', 's', ',', ' ', '&', ' ', 'l', 'i', 'k', 'e', ' ', 'm', 'a', 'n', 'y', ' ', 'w', 'h', 'o', ' ', 'd', 'o', ',', ' ', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'c', 'a', 'l', 'l', ' ', 'm', 'y', 's', 'e', 'l', 'f', ' ', 'a', 'l', 't', ' ', 'r', 'i', 'g', 'h', 't', ';', ' ', 'I', \"'\", 'm', ' ', 'a', ' ', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'i', 's', 't', ',', ' ', '&', ' ', 'n', 'o', 't', ' ', 'c', 'i', 'v', 'i', 'c', '.', ' ', 'I', \"'\", 'd', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 't', 'h', 'o', 'u', 'g', 'h', 't', ' ', \"'\", 'a', 'l', 't', ' ', 'r', 'i', 'g', 'h', 't', \"'\", ' ', 'i', 's', ' ', 'a', 'n', ' ', 'u', 'm', 'b', 'r', 'e', 'l', 'l', 'a', ' ', 't', 'e', 'r', 'm', ' ', 't', 'h', 'o', ',', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'm', 'a', 'n', 'y', ' ', 'a', 'r', 'e', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'a', 'l', 't', ' ', 'l', 'i', 't', 'e', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 't', 's', 'e', 'l', 'f', ' ', 'i', 's', ' ', 'l', 'a', 'r', 'g', 'e', 'l', 'y', ' ', 'c', 'l', 'a', 's', 's', 'i', 'c', 'a', 'l', ' ', 'l', 'i', 'b', 'e', 'r', 'a', 'l', ' ', 'o', 'r', ' ', 'c', 'i', 'v', 'i', 'c', ' ', 'n', 'a', 't', 'i', 'o', 'n', 'a', 'l', 'i', 's', 't', '.', ' ', 'T', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 'a', ' ', 'l', 'o', 't', ' ', 'o', 'f', ' ', \"'\", 'i', 'n', 'f', 'i', 'l', 't', 'r', 'a', 't', 'o', 'r', 's', \"'\", ' ', 't', 'r', 'y', 'i', 'n', 'g', ' ', 't', 'o', ' ', 's', 'o', 'w', ' ', 'd', 'i', 's', 'c', 'o', 'r', 'd', ',', ' ', 'w', '/', ' ', 's', 'o', 'm', 'e', ' ', 's', 'u', 'c', 'c', 'e', 's', 's', '.', ' ', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'l', 'i', 'k', 'e', ' ', 'i', 'n', 'f', 'i', 'g', 'h', 't', 'i', 'n', 'g', '.'], ['I', ' ', 'a', 'm', ' ', 's', 'o', ' ', 'r', 'e', 'a', 'd', 'y', ' ', 't', 'o', ' ', 'g', 'e', 't', ' ', 'b', 'a', 'c', 'k', ' ', 't', 'o', ' ', 'b', 'l', 'o', 'g', 'g', 'i', 'n', 'g', '!', ' ', 'w', 'w', 'w', '.', 'b', 'e', 'n', 'b', 'r', 'i', 'h', 'o', 'u', 's', 'e', '.', 'c', 'o', 'm', ' ', '#', 'r', 'e', 'c', 'i', 'p', 'e', 's', ' ', '#', 'f', 'o', 'o', 'd', 'p', 'h', 'o', 't', 'o', 'g', 'r', 'a', 'p', 'h', 'y']]\n"
     ]
    }
   ],
   "source": [
    "#character tokenisation\n",
    "import pandas as pd\n",
    "\n",
    "texts = train_df['text'].values\n",
    "\n",
    "# Define a function to perform character tokenization\n",
    "def character_tokenization(texts):\n",
    "    tokenized_texts = [[char for char in text] for text in texts]\n",
    "    return tokenized_texts\n",
    "# Tokenize the texts\n",
    "tokenized_texts = character_tokenization(texts)\n",
    "\n",
    "# Example of tokenized texts\n",
    "print(tokenized_texts[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
