{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cebd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01264f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file = r\"C:\\Users\\prits\\Downloads\\Data\\cleaned_ghc_train.csv\"\n",
    "test_file = r'C:\\Users\\prits\\Downloads\\Data\\cleaned_ghc_test.csv'\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67302ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo label\n",
      "0      he most likely converted to islam due to his n...   0   0   0     h\n",
      "1      so ford lied about being a psychologist record...   0   0   0     h\n",
      "2      jobs education ending abuse of nation californ...   0   0   0    nh\n",
      "3      i share a lot of your values  like many who do...   0   0   0     h\n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0    nh\n",
      "...                                                  ...  ..  ..  ..   ...\n",
      "21771  im a fan of western civilization and one bedro...   0   0   0     h\n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0     h\n",
      "21773  thank you to all my followers that follow me e...   0   0   0     h\n",
      "21774                                   wednesday music    0   0   0    nh\n",
      "21775                    this is a really big surprise     0   0   0     h\n",
      "\n",
      "[21776 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c410af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prits\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\prits\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178ed4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo label  \\\n",
      "0      he most likely converted to islam due to his n...   0   0   0     h   \n",
      "1      so ford lied about being a psychologist record...   0   0   0     h   \n",
      "2      jobs education ending abuse of nation californ...   0   0   0    nh   \n",
      "3      i share a lot of your values  like many who do...   0   0   0     h   \n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0    nh   \n",
      "...                                                  ...  ..  ..  ..   ...   \n",
      "21771  im a fan of western civilization and one bedro...   0   0   0     h   \n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0     h   \n",
      "21773  thank you to all my followers that follow me e...   0   0   0     h   \n",
      "21774                                   wednesday music    0   0   0    nh   \n",
      "21775                    this is a really big surprise     0   0   0     h   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [he, most, likely, converted, to, islam, due, ...  \n",
      "1      [so, ford, lied, about, being, a, psychologist...  \n",
      "2      [jobs, education, ending, abuse, of, nation, c...  \n",
      "3      [i, share, a, lot, of, your, values, like, man...  \n",
      "4      [i, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "21771  [im, a, fan, of, western, civilization, and, o...  \n",
      "21772  [or, is, she, saying, that, muslims, dont, kno...  \n",
      "21773  [thank, you, to, all, my, followers, that, fol...  \n",
      "21774                                 [wednesday, music]  \n",
      "21775               [this, is, a, really, big, surprise]  \n",
      "\n",
      "[21776 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#word tokenisation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "# Download the necessary NLTK data files (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize each text in the DataFrame\n",
    "train_df['tokens'] = train_df['text'].apply(word_tokenize)\n",
    "\n",
    "# Print the DataFrame with tokens\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089a07a",
   "metadata": {},
   "source": [
    " TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f74a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4785)\t0.16203292794897928\n",
      "  (0, 2901)\t0.3892354001637881\n",
      "  (0, 1335)\t0.3892354001637881\n",
      "  (0, 2337)\t0.31046282407931214\n",
      "  (0, 1774)\t0.12408419299574001\n",
      "  (0, 498)\t0.21140455796475746\n",
      "  (0, 2946)\t0.3260066957099285\n",
      "  (0, 2118)\t0.18885372906548606\n",
      "  (0, 1403)\t0.29776487683266445\n",
      "  (0, 2336)\t0.27002468653824596\n",
      "  (0, 4498)\t0.18298530207380456\n",
      "  (0, 2601)\t0.3060515584997593\n",
      "  (0, 2875)\t0.22706220952863765\n",
      "  (0, 2056)\t0.1789671016915568\n",
      "  (1, 4902)\t0.2613628716585361\n",
      "  (1, 2990)\t0.19093982502863016\n",
      "  (1, 4243)\t0.35324032765395047\n",
      "  (1, 2417)\t0.18222690689730897\n",
      "  (1, 3955)\t0.23702603418299092\n",
      "  (1, 3890)\t0.3286745616687188\n",
      "  (1, 3592)\t0.37005381211840016\n",
      "  (1, 89)\t0.18455570647653158\n",
      "  (1, 2593)\t0.3834673281604326\n",
      "  (1, 1779)\t0.35822471249323373\n",
      "  (1, 4073)\t0.18314506744675355\n",
      "  :\t:\n",
      "  (21773, 3122)\t0.2300645094202304\n",
      "  (21773, 2915)\t0.14194880498801446\n",
      "  (21773, 2348)\t0.0748393265299162\n",
      "  (21773, 466)\t0.25211483374965005\n",
      "  (21773, 2955)\t0.12316533172556458\n",
      "  (21773, 689)\t0.09300220080747736\n",
      "  (21773, 4414)\t0.14569739883406135\n",
      "  (21773, 2755)\t0.21199460084653365\n",
      "  (21773, 4881)\t0.0824840064540091\n",
      "  (21773, 205)\t0.1851767510581503\n",
      "  (21773, 2334)\t0.0625084459502674\n",
      "  (21773, 230)\t0.14125617286658268\n",
      "  (21773, 1356)\t0.10695374529698398\n",
      "  (21773, 2599)\t0.09876860501074045\n",
      "  (21773, 3950)\t0.16758377549225928\n",
      "  (21773, 3048)\t0.059187330217101576\n",
      "  (21773, 4073)\t0.0987108981034987\n",
      "  (21773, 4498)\t0.16029541569308534\n",
      "  (21774, 4811)\t0.7761907953532575\n",
      "  (21774, 2912)\t0.6304980961183604\n",
      "  (21775, 4305)\t0.6697105530989748\n",
      "  (21775, 530)\t0.4916094684422753\n",
      "  (21775, 4445)\t0.2657193122534239\n",
      "  (21775, 3575)\t0.4400168607452037\n",
      "  (21775, 2334)\t0.21350951960339454\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "texts = train_df['text'].values\n",
    "labels = train_df['label'].values\n",
    "\n",
    "# Create a mask for rows without NaN values in 'texts'\n",
    "mask = ~pd.isnull(texts)\n",
    "\n",
    "# Apply the mask to both texts and labels\n",
    "texts = texts[mask]\n",
    "labels = labels[mask]\n",
    "\n",
    "def tfidf_embedding(texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(texts)\n",
    "print(embeddings_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e98f9",
   "metadata": {},
   "source": [
    "Word2Vec is an effort to map words to high-dimensional vectors to capture the semantic relationships between words.Words with similar meanings should have similar vector representations, according to the main principle of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373e15ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12863833  0.18229553  0.03983613 ... -0.32184178 -0.06998452\n",
      "  -0.29096565]\n",
      " [ 0.01588773  0.26588994  0.19004579 ... -0.25425884 -0.08934192\n",
      "  -0.5033654 ]\n",
      " [-0.04830959  0.24815992 -0.02696376 ... -0.10394708  0.07483985\n",
      "  -0.07230787]\n",
      " ...\n",
      " [-0.09826891  0.53646976  0.35280833 ... -0.28204152 -0.09486767\n",
      "  -0.17183422]\n",
      " [-0.00709109  0.30483574 -0.04895918 ... -0.27518755  0.08552052\n",
      "  -0.19927996]\n",
      " [-0.10682079  0.2881086   0.05734314 ... -0.43156347 -0.1361157\n",
      "  -0.6704223 ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Extract texts from the training dataframe\n",
    "texts_train = train_df['text'].values\n",
    "\n",
    "# Handle any NaN values in the text column\n",
    "texts_train = np.where(pd.isnull(texts_train), '', texts_train)\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_train = [word_tokenize(text) for text in texts_train]\n",
    "\n",
    "# Train a Word2Vec model on the training texts\n",
    "w2v_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to create average Word2Vec embeddings for each text\n",
    "def get_average_word2vec(tokens_list, model, vocabulary, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "    for token in tokens_list:\n",
    "        if token in vocabulary:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[token])\n",
    "    if n_words > 0:\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "# Create embeddings for the training texts\n",
    "vocabulary = set(w2v_model.wv.index_to_key)\n",
    "embeddings_train = np.array([get_average_word2vec(tokens, w2v_model, vocabulary, 100) for tokens in tokenized_train])\n",
    "\n",
    "print(embeddings_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85c307",
   "metadata": {},
   "source": [
    "OneHotEncoding:One hot encoding is a technique that we use to represent categorical variables as numerical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2613f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  hd  cv  vo label  \\\n",
      "0  he most likely converted to islam due to his n...   0   0   0     h   \n",
      "1  so ford lied about being a psychologist record...   0   0   0     h   \n",
      "\n",
      "                                              tokens  \n",
      "0  [he, most, likely, converted, to, islam, due, ...  \n",
      "1  [so, ford, lied, about, being, a, psychologist...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21776 entries, 0 to 21775\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    21776 non-null  object\n",
      " 1   hd      21776 non-null  int64 \n",
      " 2   cv      21776 non-null  int64 \n",
      " 3   vo      21776 non-null  int64 \n",
      " 4   label   21776 non-null  object\n",
      " 5   tokens  21776 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 1020.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "print(train_df.head(2), sep = \" \")\n",
    "train_df.info()\n",
    "\n",
    "# Converting 'IsHatespeech' column to categorical to avoid Bias \n",
    "train_df['text'] = train_df['label'].astype(str)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "encoded_data = encoder.fit_transform(train_df[['label']])\n",
    "\n",
    "# Converting the encoded data into an array\n",
    "encoded_array = encoded_data.toarray()\n",
    "\n",
    "# Print the encoded data (this will show the transformed categorical data)\n",
    "encoded_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0fb54",
   "metadata": {},
   "source": [
    "Label Encoding :\n",
    "Label Encoding is a technique that is used to convert categorical columns into numerical ones so that they can be fitted by machine learning models which only take numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b96363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prits\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the categorical features\n",
    "encoded_data = label_encoder.fit_transform(train_df[['label']])\n",
    "\n",
    "print(encoded_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
