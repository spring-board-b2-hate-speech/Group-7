{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae59e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e5e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file = r\"C:\\Users\\prits\\Downloads\\Data\\cleaned_ghc_train.csv\"\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4348e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo  label\n",
      "0      he most likely converted to islam due to his n...   0   0   0      0\n",
      "1      so ford lied about being a psychologist record...   0   0   0      0\n",
      "2      jobs education ending abuse of nation californ...   0   0   0      0\n",
      "3      i share a lot of your values  like many who do...   0   0   0      0\n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0      0\n",
      "...                                                  ...  ..  ..  ..    ...\n",
      "21771  im a fan of western civilization and one bedro...   0   0   0      0\n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0      0\n",
      "21773  thank you to all my followers that follow me e...   0   0   0      0\n",
      "21774                                   wednesday music    0   0   0      0\n",
      "21775                    this is a really big surprise     0   0   0      0\n",
      "\n",
      "[21776 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0207dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prits\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prits\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prits\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37d1e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo label  \\\n",
      "0      he most likely converted to islam due to his n...   0   0   0     h   \n",
      "1      so ford lied about being a psychologist record...   0   0   0     h   \n",
      "2      jobs education ending abuse of nation californ...   0   0   0    nh   \n",
      "3      i share a lot of your values  like many who do...   0   0   0     h   \n",
      "4      i am so ready to get back to blogging  recipes...   0   0   0    nh   \n",
      "...                                                  ...  ..  ..  ..   ...   \n",
      "21771  im a fan of western civilization and one bedro...   0   0   0     h   \n",
      "21772  or  is she saying that muslims dont know how t...   0   0   0     h   \n",
      "21773  thank you to all my followers that follow me e...   0   0   0     h   \n",
      "21774                                   wednesday music    0   0   0    nh   \n",
      "21775                    this is a really big surprise     0   0   0     h   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [he, most, likely, converted, to, islam, due, ...  \n",
      "1      [so, ford, lied, about, being, a, psychologist...  \n",
      "2      [jobs, education, ending, abuse, of, nation, c...  \n",
      "3      [i, share, a, lot, of, your, values, like, man...  \n",
      "4      [i, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "21771  [im, a, fan, of, western, civilization, and, o...  \n",
      "21772  [or, is, she, saying, that, muslims, dont, kno...  \n",
      "21773  [thank, you, to, all, my, followers, that, fol...  \n",
      "21774                                 [wednesday, music]  \n",
      "21775               [this, is, a, really, big, surprise]  \n",
      "\n",
      "[21776 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#word tokenisation\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "# Download the necessary NLTK data files (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize each text in the DataFrame\n",
    "train_df['tokens'] = train_df['text'].apply(word_tokenize)\n",
    "\n",
    "# Print the DataFrame with tokens\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f3c4e",
   "metadata": {},
   "source": [
    "OneHotEncoding:One hot encoding is a technique that we use to represent categorical variables as numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cbd3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tokens  \\\n",
      "0      [he, most, likely, converted, to, islam, due, ...   \n",
      "1      [so, ford, lied, about, being, a, psychologist...   \n",
      "2      [jobs, education, ending, abuse, of, nation, c...   \n",
      "3      [i, share, a, lot, of, your, values, like, man...   \n",
      "4      [i, am, so, ready, to, get, back, to, blogging...   \n",
      "...                                                  ...   \n",
      "21771  [im, a, fan, of, western, civilization, and, o...   \n",
      "21772  [or, is, she, saying, that, muslims, dont, kno...   \n",
      "21773  [thank, you, to, all, my, followers, that, fol...   \n",
      "21774                                 [wednesday, music]   \n",
      "21775               [this, is, a, really, big, surprise]   \n",
      "\n",
      "                                         one_hot_encoded  \n",
      "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                  ...  \n",
      "21771  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "21772  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "21773  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "21774  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "21775  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[21776 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = train_df\n",
    "\n",
    "# Extract vocabulary\n",
    "vocabulary = set()\n",
    "_ = df['tokens'].apply(lambda x: vocabulary.update(x))\n",
    "vocab_list = list(vocabulary)\n",
    "\n",
    "# Function for one-hot encoding\n",
    "def one_hot_encode(tokens, vocab_list):\n",
    "    encoding = [0] * len(vocab_list)\n",
    "    for token in tokens:\n",
    "        if token in vocab_list:\n",
    "            index = vocab_list.index(token)\n",
    "            encoding[index] = 1\n",
    "    return encoding\n",
    "\n",
    "# Apply one-hot encoding\n",
    "df['one_hot_encoded'] = df['tokens'].apply(lambda x: one_hot_encode(x, vocab_list))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df[['tokens', 'one_hot_encoded']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9905c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['one_hot_encoded'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Prepare the features and labels\n",
    "X = df['one_hot_encoded'].tolist()\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='h', average='binary')\n",
    "recall = recall_score(y_test, y_pred, pos_label='h', average='binary')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='h', average='binary')\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d83b7d",
   "metadata": {},
   "source": [
    "Label Encoding:Label Encoding is a technique that is used to convert categorical columns into numerical ones so that they can be fitted by machine learning models which only take numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80674b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['one_hot_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to each column that is categorical\n",
    "categorical_columns = ['text', 'one_hot_encoded']\n",
    "for col in categorical_columns:\n",
    "    train_df[col] = encoder.fit_transform(train_df[col])\n",
    "x = train_df['text'].values\n",
    "y = train_df['one_hot_encoded'].values\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "# Reshape X_train and X_test if needed\n",
    "X_train = X_train.reshape(-1, 1)  # Example reshape for X_train, adjust as per your data structure\n",
    "X_test = X_test.reshape(-1, 1)    # Example reshape for X_test, adjust as per your data structure\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c41679",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14df6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4785)\t0.1620296940882516\n",
      "  (0, 2905)\t0.38923917472124503\n",
      "  (0, 1332)\t0.38923917472124503\n",
      "  (0, 2343)\t0.3104641687725634\n",
      "  (0, 1773)\t0.12407978854649923\n",
      "  (0, 492)\t0.21140284704973647\n",
      "  (0, 2951)\t0.32600851987838764\n",
      "  (0, 2126)\t0.18885132253462167\n",
      "  (0, 1400)\t0.2977658298377288\n",
      "  (0, 2342)\t0.27002478385343953\n",
      "  (0, 4503)\t0.18297448250041176\n",
      "  (0, 2606)\t0.30605276712057433\n",
      "  (0, 2879)\t0.22706098159854585\n",
      "  (0, 2065)\t0.17896439019209592\n",
      "  (1, 4902)\t0.26136219694263846\n",
      "  (1, 2996)\t0.19093692873719764\n",
      "  (1, 4246)\t0.3532425513174643\n",
      "  (1, 2420)\t0.18222373574691075\n",
      "  (1, 3958)\t0.23702459173372586\n",
      "  (1, 3892)\t0.3286760103770535\n",
      "  (1, 3598)\t0.37005656618250765\n",
      "  (1, 89)\t0.1845526087907791\n",
      "  (1, 2596)\t0.38347050536924204\n",
      "  (1, 1779)\t0.35822709339486697\n",
      "  (1, 4078)\t0.18314192526077977\n",
      "  :\t:\n",
      "  (21769, 3128)\t0.23006431738505032\n",
      "  (21769, 2919)\t0.14194981143002833\n",
      "  (21769, 2353)\t0.07483758427314527\n",
      "  (21769, 462)\t0.2521107373031192\n",
      "  (21769, 2960)\t0.12316556882602916\n",
      "  (21769, 683)\t0.0930012024734965\n",
      "  (21769, 4420)\t0.14569375125460238\n",
      "  (21769, 2763)\t0.21199366869629266\n",
      "  (21769, 4881)\t0.08248257731131536\n",
      "  (21769, 200)\t0.18517472049092235\n",
      "  (21769, 2340)\t0.06250619864000412\n",
      "  (21769, 223)\t0.14125715093947327\n",
      "  (21769, 1353)\t0.1069533183963426\n",
      "  (21769, 2604)\t0.0987678428596147\n",
      "  (21769, 3953)\t0.1675858319023834\n",
      "  (21769, 3054)\t0.059184946879156514\n",
      "  (21769, 4078)\t0.09871013358878872\n",
      "  (21769, 4503)\t0.1602875584674718\n",
      "  (21770, 4811)\t0.7761924236019886\n",
      "  (21770, 2916)\t0.630496091615857\n",
      "  (21771, 4309)\t0.6697170126868022\n",
      "  (21771, 524)\t0.4916098432207747\n",
      "  (21771, 4451)\t0.2657119695127845\n",
      "  (21771, 3581)\t0.440015472866383\n",
      "  (21771, 2340)\t0.21350039311936056\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_df.dropna(subset=['text'], inplace=True)\n",
    "def tfidf_embedding(text):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(text)\n",
    "    return embeddings\n",
    "\n",
    "text = train_df['text'].values\n",
    "embeddings_tfidf = tfidf_embedding(text)\n",
    "print(embeddings_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386be9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8840\n",
      "Precision: 0.8688\n",
      "Recall: 0.8840\n",
      "F1 Score: 0.8476\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#Step 1: TF-IDF Encoding\n",
    "def tfidf_embedding(text):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(text)\n",
    "    return embeddings\n",
    "\n",
    "text = train_df['text'].values\n",
    "label = train_df['label'].values\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(text)\n",
    "\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_tfidf, hd, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predictions and Evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7db01",
   "metadata": {},
   "source": [
    "Word2Vec is an effort to map words to high-dimensional vectors to capture the semantic relationships between words.Words with similar meanings should have similar vector representations, according to the main principle of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74014f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'islam':\n",
      "[-0.71788096  0.69635594 -1.2575378  -1.6985968   1.243707   -3.5326726\n",
      " -0.05413126  0.4291866   0.4127708  -0.3037556   0.44572082  0.5336933\n",
      " -3.41262    -1.5923476   2.0906234  -0.6998179  -1.3351296  -0.5787967\n",
      "  0.88346463  2.0686119  -1.368465   -0.8264294   1.7392495   1.0691624\n",
      "  0.5093605  -0.7518689  -0.45030582  2.877648    1.451578    2.047943\n",
      "  0.48797953 -1.2455983  -0.9005894   2.3584192   1.058602   -0.0930863\n",
      " -1.1195761  -0.9972075  -1.1392921  -3.1992702  -0.2339263  -1.2074083\n",
      " -0.28990424 -0.39624316 -1.8081695   1.0093127  -1.1810701   0.30767867\n",
      "  0.1828549  -0.15102443  0.47526464 -1.3916031   2.2317877  -2.0066283\n",
      " -0.9464777  -1.065702    2.1249213  -1.0707022  -1.9709661  -1.2895125\n",
      "  0.6868337   1.3403411   0.05227742 -1.3027475  -1.0444919   0.55692923\n",
      "  0.1498853  -0.5426461   2.1245852   3.2971604  -3.138048   -0.51799136\n",
      "  0.7705057  -0.38215604  1.4643627   0.80813795 -0.84231925 -2.166167\n",
      " -0.44995347  0.03456383  1.0064342   1.128574   -1.2264795  -2.8064957\n",
      " -1.2809174  -1.1510165  -1.3178064   3.8946064   1.3818616  -0.9666798\n",
      "  1.6858774   0.05736826 -0.47263607 -2.9032528  -0.72791284 -0.65631753\n",
      "  0.89716744 -1.7912608   1.827816   -1.4594364 ]\n",
      "Most similar words to 'islam':\n",
      "[('religion', 0.6016958951950073), ('muslims', 0.5745822787284851), ('ideology', 0.5514807105064392), ('christianity', 0.5320969223976135), ('secular', 0.5264363288879395), ('radicalislam', 0.525892972946167), ('precipitated', 0.5253273844718933), ('buti', 0.5167506337165833), ('pagan', 0.5146461129188538), ('lense', 0.5090538263320923)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "train_df =train_df.dropna(subset=['text'])\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "text_data = train_df['text'].astype(str).fillna('').tolist()\n",
    "\n",
    "# Remove punctuation and tokenize the sentences into words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic tokens\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = [preprocess_text(sentence) for sentence in text_data if sentence.strip() != '']\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4, epochs=50)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Test the model\n",
    "word = 'islam'  # Replace 'islam' with a word from your vocabulary\n",
    "if word in model.wv:\n",
    "    print(f\"Vector representation of '{word}':\\n{model.wv[word]}\")\n",
    "    print(f\"Most similar words to '{word}':\\n{model.wv.most_similar(word)}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b8dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8753\n",
      "Precision: 0.8445\n",
      "Recall: 0.8753\n",
      "F1 Score: 0.8279\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure you have train_df['label'] as your target labels\n",
    "# Convert sentences to vectors by averaging word vectors\n",
    "def sentence_to_vector(sentence, model):\n",
    "    tokens = preprocess_text(sentence)\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X = np.array([sentence_to_vector(sentence, model) for sentence in text_data])\n",
    "y = train_df['label'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
