{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664b3e2c-0c2f-45fe-810b-28f5615b7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Load the TSV files\n",
    "train_file_path = r'C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.tsv'\n",
    "test_file_path = r'C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.tsv'\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "test_data = pd.read_csv(test_file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dcb0d-9a64-4acf-a76b-68854ff849a3",
   "metadata": {},
   "source": [
    "TOKENIZATION (Using NLTK (Natural Language Toolkit)). NLTK is a popular library for text processing and provides various tokenizers for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d4a03c-2759-47de-b317-b3185163e079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo  \\\n",
      "0      He most likely converted to islam due to his n...   0   0   0   \n",
      "1      So Ford lied about being a psychologist. Recor...   0   0   0   \n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   0   0   0   \n",
      "3      I share a lot of your values, & like many who ...   0   0   0   \n",
      "4      I am so ready to get back to blogging! www.ben...   0   0   0   \n",
      "...                                                  ...  ..  ..  ..   \n",
      "22031  I'm a fan of western civilization, and one bed...   0   0   0   \n",
      "22032  Or ... is she saying that Muslims don't know h...   0   0   0   \n",
      "22033  Thank you to all my followers that follow me e...   0   0   0   \n",
      "22034  Wednesday music. https://www.youtube.com/watch...   0   0   0   \n",
      "22035  This is a really Big Surprise!  https://www.wn...   0   0   0   \n",
      "\n",
      "                                          tokenized_text  \n",
      "0      [He, most, likely, converted, to, islam, due, ...  \n",
      "1      [So, Ford, lied, about, being, a, psychologist...  \n",
      "2      [Jobs, ., Education, ., Ending, abuse, of, Nat...  \n",
      "3      [I, share, a, lot, of, your, values, ,, &, lik...  \n",
      "4      [I, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "22031  [I, 'm, a, fan, of, western, civilization, ,, ...  \n",
      "22032  [Or, ..., is, she, saying, that, Muslims, do, ...  \n",
      "22033  [Thank, you, to, all, my, followers, that, fol...  \n",
      "22034  [Wednesday, music, ., https, :, //www.youtube....  \n",
      "22035  [This, is, a, really, Big, Surprise, !, https,...  \n",
      "\n",
      "[22036 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example DataFrame\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# Tokenize a specific column using NLTK\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['tokenized_text'] = df['text'].apply(tokenize_text)\n",
    "\n",
    "# Print the DataFrame with tokenized text\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e85e80-a2c9-4975-bb12-8e4357ab077e",
   "metadata": {},
   "source": [
    "Using Regular Expressions (Regex)\n",
    "Regex can be used for more customized tokenization based on specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a6b642-1a8b-4c21-87a8-bd20c92876cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hd  cv  vo  \\\n",
      "0      He most likely converted to islam due to his n...   0   0   0   \n",
      "1      So Ford lied about being a psychologist. Recor...   0   0   0   \n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   0   0   0   \n",
      "3      I share a lot of your values, & like many who ...   0   0   0   \n",
      "4      I am so ready to get back to blogging! www.ben...   0   0   0   \n",
      "...                                                  ...  ..  ..  ..   \n",
      "22031  I'm a fan of western civilization, and one bed...   0   0   0   \n",
      "22032  Or ... is she saying that Muslims don't know h...   0   0   0   \n",
      "22033  Thank you to all my followers that follow me e...   0   0   0   \n",
      "22034  Wednesday music. https://www.youtube.com/watch...   0   0   0   \n",
      "22035  This is a really Big Surprise!  https://www.wn...   0   0   0   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [He, most, likely, converted, to, islam, due, ...  \n",
      "1      [So, Ford, lied, about, being, a, psychologist...  \n",
      "2      [Jobs, Education, Ending, abuse, of, Nation, C...  \n",
      "3      [I, share, a, lot, of, your, values, like, man...  \n",
      "4      [I, am, so, ready, to, get, back, to, blogging...  \n",
      "...                                                  ...  \n",
      "22031  [I, m, a, fan, of, western, civilization, and,...  \n",
      "22032  [Or, is, she, saying, that, Muslims, don, t, k...  \n",
      "22033  [Thank, you, to, all, my, followers, that, fol...  \n",
      "22034  [Wednesday, music, https, www, youtube, com, w...  \n",
      "22035  [This, is, a, really, Big, Surprise, https, ww...  \n",
      "\n",
      "[22036 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Example DataFrame\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "df = pd.DataFrame(train_data)\n",
    "# Tokenization using regular expressions\n",
    "df['tokens'] = df['text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
    "\n",
    "# Print the DataFrame with tokens\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba7371-3c70-46f5-9768-0f3e5d8a0e54",
   "metadata": {},
   "source": [
    "One-hot encoding is a technique used in machine learning to convert categorical variables into a format that can be provided to machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3713e520-88ad-44d8-b314-63b53e86c7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "                                                    text  hd  cv  vo\n",
      "0      He most likely converted to islam due to his n...   0   0   0\n",
      "1      So Ford lied about being a psychologist. Recor...   0   0   0\n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   0   0   0\n",
      "3      I share a lot of your values, & like many who ...   0   0   0\n",
      "4      I am so ready to get back to blogging! www.ben...   0   0   0\n",
      "...                                                  ...  ..  ..  ..\n",
      "22031  I'm a fan of western civilization, and one bed...   0   0   0\n",
      "22032  Or ... is she saying that Muslims don't know h...   0   0   0\n",
      "22033  Thank you to all my followers that follow me e...   0   0   0\n",
      "22034  Wednesday music. https://www.youtube.com/watch...   0   0   0\n",
      "22035  This is a really Big Surprise!  https://www.wn...   0   0   0\n",
      "\n",
      "[22036 rows x 4 columns]\n",
      "\n",
      "One-hot encoded DataFrame:\n",
      "                                                    text  vo  hd_0   hd_1  \\\n",
      "0      He most likely converted to islam due to his n...   0  True  False   \n",
      "1      So Ford lied about being a psychologist. Recor...   0  True  False   \n",
      "2         Jobs. Education. Ending abuse of Nation. CA43.   0  True  False   \n",
      "3      I share a lot of your values, & like many who ...   0  True  False   \n",
      "4      I am so ready to get back to blogging! www.ben...   0  True  False   \n",
      "...                                                  ...  ..   ...    ...   \n",
      "22031  I'm a fan of western civilization, and one bed...   0  True  False   \n",
      "22032  Or ... is she saying that Muslims don't know h...   0  True  False   \n",
      "22033  Thank you to all my followers that follow me e...   0  True  False   \n",
      "22034  Wednesday music. https://www.youtube.com/watch...   0  True  False   \n",
      "22035  This is a really Big Surprise!  https://www.wn...   0  True  False   \n",
      "\n",
      "       cv_0   cv_1  \n",
      "0      True  False  \n",
      "1      True  False  \n",
      "2      True  False  \n",
      "3      True  False  \n",
      "4      True  False  \n",
      "...     ...    ...  \n",
      "22031  True  False  \n",
      "22032  True  False  \n",
      "22033  True  False  \n",
      "22034  True  False  \n",
      "22035  True  False  \n",
      "\n",
      "[22036 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# Perform one-hot encoding on the 'hd' and 'cv' columns\n",
    "df_encoded = pd.get_dummies(df, columns=['hd', 'cv'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nOne-hot encoded DataFrame:\")\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5df6b718-0c74-40c8-acbf-a6d86e391f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 rows after one-hot encoding:\n",
      "                                                 text  vo   hd_0   hd_1  cv_0  \\\n",
      "0   He most likely converted to islam due to his n...   0   True  False  True   \n",
      "1   So Ford lied about being a psychologist. Recor...   0   True  False  True   \n",
      "2      Jobs. Education. Ending abuse of Nation. CA43.   0   True  False  True   \n",
      "3   I share a lot of your values, & like many who ...   0   True  False  True   \n",
      "4   I am so ready to get back to blogging! www.ben...   0   True  False  True   \n",
      "5   taking a look at new opportunity called FX Pro...   0   True  False  True   \n",
      "6   Reflecting back when I was in school with Spec...   0   True  False  True   \n",
      "7   Let's be honest everyone, last year there were...   0   True  False  True   \n",
      "8                           2007 Nuke plant in Syria    0   True  False  True   \n",
      "9   NBC's Chuck Todd Thinks He's Figured It All Ou...   0   True  False  True   \n",
      "10                                     Nice one, lol    0   True  False  True   \n",
      "11  they must fail this midterm. Our republic depe...   0   True  False  True   \n",
      "12  'A confidential, 49-page memo for defeating Tr...   0   True  False  True   \n",
      "13  I was walking down the street with my husband ...   0   True  False  True   \n",
      "14  Suggestion: Create another channel not associa...   0   True  False  True   \n",
      "15                                           #camping   0   True  False  True   \n",
      "16                      Constitutional Conservatives    0   True  False  True   \n",
      "17  FB won't let me post the article that goes wit...   0   True  False  True   \n",
      "18                            Which Way Western Man..   0   True  False  True   \n",
      "19           Thanks, go Trump go ride the Trump train   0   True  False  True   \n",
      "20  to have flattering articles written about him ...   0   True  False  True   \n",
      "21  Democrats are tribal until a real socialist is...   0   True  False  True   \n",
      "22  Click on the live link for the latest and grea...   0   True  False  True   \n",
      "23  Go to qanon.pub for #QAnon posts coming fast a...   0   True  False  True   \n",
      "24  We are ALL tired of waiting . . realize someth...   0   True  False  True   \n",
      "25                                >>jews are at bay     0   True  False  True   \n",
      "26  https://thegoldwater.com/news/29277-Canada-Tru...   0   True  False  True   \n",
      "27  leave that shithole. let them kill each other....   0   True  False  True   \n",
      "28  do i have to take issue with it? i was enjoyin...   0   True  False  True   \n",
      "29  Saudi Money rules the Uk while Zionist Money r...   0  False   True  True   \n",
      "\n",
      "     cv_1  \n",
      "0   False  \n",
      "1   False  \n",
      "2   False  \n",
      "3   False  \n",
      "4   False  \n",
      "5   False  \n",
      "6   False  \n",
      "7   False  \n",
      "8   False  \n",
      "9   False  \n",
      "10  False  \n",
      "11  False  \n",
      "12  False  \n",
      "13  False  \n",
      "14  False  \n",
      "15  False  \n",
      "16  False  \n",
      "17  False  \n",
      "18  False  \n",
      "19  False  \n",
      "20  False  \n",
      "21  False  \n",
      "22  False  \n",
      "23  False  \n",
      "24  False  \n",
      "25  False  \n",
      "26  False  \n",
      "27  False  \n",
      "28  False  \n",
      "29  False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_file_path points to your CSV file containing data\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "\n",
    "# Perform one-hot encoding on the 'hd' and 'cv' columns\n",
    "df_encoded = pd.get_dummies(train_data, columns=['hd', 'cv'])\n",
    "\n",
    "# Get the first 100 rows\n",
    "first_30_rows = df_encoded.head(30)\n",
    "\n",
    "print(\"First 30 rows after one-hot encoding:\")\n",
    "print(first_30_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41f883-251b-4fca-856e-424e8418a69f",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in natural language processing and information retrieval to evaluate the importance of a word within a document relative to a collection of documents (corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e17036-efe3-40da-b438-c229e8c38375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2066)\t0.17380003372099462\n",
      "  (0, 2896)\t0.22878138778366716\n",
      "  (0, 2611)\t0.308292698278812\n",
      "  (0, 4505)\t0.18450177248891259\n",
      "  (0, 2356)\t0.2708265996650353\n",
      "  (0, 1410)\t0.3000583507165376\n",
      "  (0, 2126)\t0.19041183869663905\n",
      "  (0, 2969)\t0.3249005733982352\n",
      "  (0, 513)\t0.21300835847911828\n",
      "  (0, 1782)\t0.1251958019134632\n",
      "  (0, 2357)\t0.3097041841718857\n",
      "  (0, 1342)\t0.3928305333185176\n",
      "  (0, 2921)\t0.38098114710475606\n",
      "  (0, 4811)\t0.1635314267617231\n",
      "  (1, 4505)\t0.09971825143742916\n",
      "  (1, 513)\t0.2302505906860185\n",
      "  (1, 4811)\t0.17676868586969255\n",
      "  (1, 4087)\t0.18327460052796524\n",
      "  (1, 1787)\t0.35341952345896444\n",
      "  (1, 2602)\t0.3861364326507394\n",
      "  (1, 105)\t0.1847225247424156\n",
      "  (1, 3624)\t0.3709947560050697\n",
      "  (1, 3905)\t0.33104507543019485\n",
      "  (1, 3972)\t0.23144924647032356\n",
      "  (1, 2434)\t0.18217361586277575\n",
      "  :\t:\n",
      "  (22033, 3120)\t0.19353958307276067\n",
      "  (22034, 4960)\t0.2558599078555277\n",
      "  (22034, 943)\t0.2301032171912457\n",
      "  (22034, 2191)\t0.21703677481126502\n",
      "  (22034, 4816)\t0.3302014304252174\n",
      "  (22034, 4987)\t0.33574609238590625\n",
      "  (22034, 2933)\t0.5001472906938732\n",
      "  (22034, 4836)\t0.6021441021667792\n",
      "  (22035, 2354)\t0.10337503833351554\n",
      "  (22035, 3607)\t0.21318013021636575\n",
      "  (22035, 4960)\t0.14533491919897415\n",
      "  (22035, 943)\t0.130704465417055\n",
      "  (22035, 3075)\t0.21429289456176567\n",
      "  (22035, 2191)\t0.12328239463062748\n",
      "  (22035, 4452)\t0.12876157966188936\n",
      "  (22035, 4088)\t0.25042382008188857\n",
      "  (22035, 2782)\t0.22376348623422684\n",
      "  (22035, 42)\t0.17874970218314531\n",
      "  (22035, 9)\t0.23376526259691963\n",
      "  (22035, 2748)\t0.37612892723651753\n",
      "  (22035, 546)\t0.23021744610087005\n",
      "  (22035, 3736)\t0.3365051415404603\n",
      "  (22035, 2476)\t0.3082961981596739\n",
      "  (22035, 803)\t0.3483728014249454\n",
      "  (22035, 4311)\t0.3232083707407497\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "df = pd.DataFrame(train_data)\n",
    "text = train_data['text'].values\n",
    "hd = train_data['hd'].values\n",
    "def tfidf_embedding(text):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(text)\n",
    "    return embeddings\n",
    "embeddings_tfidf = tfidf_embedding(text)\n",
    "print(embeddings_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed62b52-fc62-4030-ad56-3125d5026d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9195\n",
      "Precision: 0.9023\n",
      "Recall: 0.9195\n",
      "F1 Score: 0.8879\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "\n",
    "#Step 1: TF-IDF Encoding\n",
    "def tfidf_embedding(text):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    embeddings = vectorizer.fit_transform(text)\n",
    "    return embeddings\n",
    "\n",
    "text = train_data['text'].values\n",
    "hd = train_data['hd'].values\n",
    "\n",
    "embeddings_tfidf = tfidf_embedding(text)\n",
    "\n",
    "# Step 2: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_tfidf, hd, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predictions and Evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af89205-c1cd-4106-b34e-b0e73c143d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
