{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44367a9a-2d6d-46a8-9303-f1941e5d955e",
   "metadata": {},
   "source": [
    "#LABEL ENCODING\n",
    "#Label Encoding: Assign each categorical value an integer value based on alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e45edd-4321-429b-bfab-7eaa6e9cb39f",
   "metadata": {},
   "source": [
    "#ONE HOT ENCODING\n",
    "#One Hot Encoding: One hot encoding is a technique that we use to represent categorical variables as numerical values in a machine learning model. \n",
    "#Create new variables that take on values 0 and 1 to represent the original categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703f4cb-8833-4ad3-9177-6e7a77600dd4",
   "metadata": {},
   "source": [
    "#TF-IDF(TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY)\n",
    "#TF-IDF quantifies the importance of a word in a document relative to a corpus by considering both the frequency of the word in the document\n",
    "#and its rarity across all documents. It's commonly used in information retrieval and natural language processing tasks to rank words by their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2fc20-49c1-4665-8343-a56fcf95a384",
   "metadata": {},
   "source": [
    "#WORD2VEC\n",
    "#Word2Vec is a widely used method in NLP that allows words to be represented as vectors in a continuous vector space.\n",
    "#The CBOW model predicts the current word given context words within a specific window.\n",
    "#Skip gram predicts the surrounding context words within specific window given current word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc50bf1-c029-4614-a818-feee1e09eac2",
   "metadata": {},
   "source": [
    "#TOKENIZATION TECHNIQUES\n",
    "Tokenization is a fundamental process in natural language processing (NLP) and involves breaking down text into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the tokenization technique used. Here are some common tokenization techniques:\n",
    "\n",
    "1. Whitespace Tokenization\n",
    "This is the simplest form of tokenization, where tokens are created by splitting the text at whitespace (spaces, tabs, newlines).\n",
    "Example: \"Hello, world!\" becomes [\"Hello,\", \"world!\"]\n",
    "\n",
    "2. Punctuation-Based Tokenization\n",
    "This method involves splitting the text not only at whitespace but also at punctuation marks.\n",
    "Example: \"Hello, world!\" becomes [\"Hello\", \",\", \"world\", \"!\"]\n",
    "\n",
    "3. Word Tokenization\n",
    "Tokens are individual words. This method may include handling punctuation, contractions, and other language-specific nuances.\n",
    "Example: \"I'm learning NLP.\" becomes [\"I'm\", \"learning\", \"NLP\", \".\"]\n",
    "\n",
    "4. Subword Tokenization\n",
    "Subword tokenization breaks words into smaller units or subwords. This technique is useful for handling out-of-vocabulary words in language models.\n",
    "\n",
    "Byte Pair Encoding (BPE): Merges the most frequent pairs of characters or character sequences iteratively.\n",
    "Example: \"lower\" might be split into [\"low\", \"er\"].\n",
    "\n",
    "WordPiece: Similar to BPE but used in models like BERT. It generates subwords based on the frequency of occurrence in a corpus.\n",
    "Example: \"unhappiness\" might be split into [\"un\", \"##happy\", \"##ness\"].\n",
    "\n",
    "5. Character Tokenization\n",
    "Each character in the text is treated as a token. This is useful for languages with complex morphology or when dealing with misspellings and typos.\n",
    "Example: \"NLP\" becomes [\"N\", \"L\", \"P\"]\n",
    "\n",
    "6. Sentence Tokenization\n",
    "This technique involves splitting the text into sentences, typically using punctuation marks like periods, exclamation points, and question marks.\n",
    "Example: \"Hello world! How are you?\" becomes [\"Hello world!\", \"How are you?\"]\n",
    "\n",
    "7. Rule-Based Tokenization\n",
    "Uses predefined rules to split text into tokens. These rules can be based on regular expressions or linguistic patterns.\n",
    "Example: In medical text, rules might be defined to handle abbreviations, chemical formulas, and medical terminology correctly.\n",
    "\n",
    "8. Tokenizer APIs and Libraries\n",
    "There are several libraries and tools available for tokenization that handle various languages and complexities:\n",
    "\n",
    "NLTK (Natural Language Toolkit): Provides simple to advanced tokenization methods.\n",
    "\n",
    "spaCy: Offers efficient and accurate tokenization for multiple languages.\n",
    "\n",
    "Transformers (Hugging Face): Includes tokenizers for various pre-trained models like BERT, GPT, etc.\n",
    "\n",
    "9. Hybrid Tokenization\n",
    "Combines multiple tokenization strategies to handle specific use cases or languages more effectively.\n",
    "Example: Combining word and subword tokenization for better handling of compound words in German."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
