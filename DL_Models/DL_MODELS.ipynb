{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010b5d74-eafc-4996-8bc9-843541cd7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.csv\"\n",
    "test_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.csv\"\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df=pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63987a2-9f97-4cd2-bd22-8abf7dabdc04",
   "metadata": {},
   "source": [
    "# Preprocessing and Tokenizing for DL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf1059b-80cc-4916-a6f7-861f3c1a1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to map original labels to binary values\n",
    "def map_labels(df):\n",
    "    label_mapping = {'__label__0': 0, '__label__1': 1}\n",
    "    if 'label' in df.columns:\n",
    "        df['label'] = df['label'].map(label_mapping)\n",
    "    return df\n",
    "\n",
    "# Function to ensure text column is string type and handle missing values\n",
    "def preprocess_text_column(df):\n",
    "    df['text'] = df['text'].fillna('').astype(str)\n",
    "    return df\n",
    "\n",
    "# File paths\n",
    "train_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.csv\"\n",
    "test_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.csv\"\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "# Apply functions to preprocess train_df and test_df\n",
    "train_df = map_labels(train_df)\n",
    "test_df = map_labels(test_df)\n",
    "\n",
    "train_df = preprocess_text_column(train_df)\n",
    "test_df = preprocess_text_column(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a4b1f4-0ed8-40fa-9a3e-2bc1ae28f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified train_df:\n",
      "                                                text  hd  cv  vo  label\n",
      "0  he most likely converted to islam due to his n...   0   0   0      0\n",
      "1  so ford lied about being a psychologist. recor...   0   0   0      0\n",
      "2     jobs. education. ending abuse of nation. ca43.   0   0   0      0\n",
      "3  i share a lot of your values, & like many who ...   0   0   0      0\n",
      "4  i am so ready to get back to blogging! www.ben...   0   0   0      0\n",
      "\n",
      "Modified test_df:\n",
      "                                                text  hd  cv  vo  label\n",
      "0  https://www.youtube.com/watch?v=kacwpkaktak a ...   0   0   0      0\n",
      "1  very nice! i tend to get tired of the constant...   0   0   0      0\n",
      "2        watch today. https://circumcisionmovie.com/   0   0   0      0\n",
      "3  \" thinking venues \" first color layer blocking...   0   0   0      0\n",
      "4  what about death penalty for perpetrators  and...   0   0   0      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert text column to lowercase\n",
    "def convert_to_lower(df, text):\n",
    "    df[text] = df[text].str.lower()\n",
    "    return df\n",
    "\n",
    "# File paths\n",
    "train_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.csv\"\n",
    "test_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.csv\"\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "# Apply convert_to_lower function to 'text' column in train_df and test_df\n",
    "train_df = convert_to_lower(train_df, 'text')\n",
    "test_df = convert_to_lower(test_df, 'text')\n",
    "\n",
    "# Example of using the modified train_df and test_df\n",
    "print(\"Modified train_df:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nModified test_df:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ad78ac-5264-4176-aac3-d6c6c99f3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to tokenize text data\n",
    "def tokenize_texts(train_texts, test_texts, num_words=None):\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "    \n",
    "    return tokenizer, X_train_seq, X_test_seq\n",
    "\n",
    "# Function to pad sequences\n",
    "def pad_sequences_data(X_train_seq, X_test_seq, max_sequence_length):\n",
    "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "    return X_train_padded, X_test_padded\n",
    "\n",
    "# Function to extract labels\n",
    "def extract_labels(train_data, test_data):\n",
    "    y_train = train_data['label'].values\n",
    "    y_test = test_data['label'].values\n",
    "    return y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b642c2-5ab1-48d5-88f9-e8a7217441c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.csv\"\n",
    "test_file = r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.csv\"\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "# Example usage\n",
    "tokenizer, X_train_seq, X_test_seq = tokenize_texts(train_df['text'], test_df['text'], num_words=5000)\n",
    "X_train_padded, X_test_padded = pad_sequences_data(X_train_seq, X_test_seq, max_sequence_length=100)\n",
    "y_train, y_test = extract_labels(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb13729-5a38-4fbc-b248-73e38015c72f",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e63082-11f7-47b0-9879-73ada1067e71",
   "metadata": {},
   "source": [
    "LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem that can occur when training traditional RNNs. It is particularly effective for modeling sequential data, where each element in the sequence depends on previous elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4bb8b70-721c-4bbc-bb60-706468c47ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHARGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 208ms/step - accuracy: 0.8771 - loss: 0.3572 - val_accuracy: 0.8907 - val_loss: 0.2847\n",
      "Epoch 2/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 207ms/step - accuracy: 0.9064 - loss: 0.2473 - val_accuracy: 0.8909 - val_loss: 0.2895\n",
      "Epoch 3/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 206ms/step - accuracy: 0.9207 - loss: 0.2044 - val_accuracy: 0.8869 - val_loss: 0.3156\n",
      "Epoch 4/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 210ms/step - accuracy: 0.9348 - loss: 0.1679 - val_accuracy: 0.8829 - val_loss: 0.3324\n",
      "Epoch 5/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 206ms/step - accuracy: 0.9467 - loss: 0.1403 - val_accuracy: 0.8733 - val_loss: 0.3893\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 47ms/step - accuracy: 0.8695 - loss: 0.3929\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step\n",
      "LSTM Model Accuracy: 0.8733212351799011\n",
      "LSTM Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.95      0.93      4803\n",
      "     Class 1       0.51      0.38      0.43       707\n",
      "\n",
      "    accuracy                           0.87      5510\n",
      "   macro avg       0.71      0.66      0.68      5510\n",
      "weighted avg       0.86      0.87      0.87      5510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense, SpatialDropout1D, Bidirectional\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.csv\")\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=100, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=100, padding='post')\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Define LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "# Remove dropout after Embedding layer\n",
    "\n",
    "# Add Bidirectional LSTM layer\n",
    "lstm_model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "# Add Dropout layer after Bidirectional LSTM\n",
    "lstm_model.add(Dropout(0.2))\n",
    "\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_padded, y_test)\n",
    "lstm_y_pred = lstm_model.predict(X_test_padded)\n",
    "lstm_y_pred_classes = (lstm_y_pred > 0.5).astype(int).flatten()  # Convert probabilities to binary classes\n",
    "lstm_report = classification_report(y_test, lstm_y_pred_classes, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "print(\"LSTM Model Accuracy:\", lstm_accuracy)\n",
    "print(\"LSTM Model Classification Report:\\n\", lstm_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535fdc9-8abe-4b77-86d8-0d3e8acf0731",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45d727-a46a-4eb5-b229-11ae60a54296",
   "metadata": {},
   "source": [
    "CNNs offer a powerful approach to text classification by leveraging their ability to learn hierarchical representations of sequential data. While traditionally associated with computer vision tasks, their adaptation to NLP tasks like text classification has proven effective, particularly when dealing with large datasets and complex patterns in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5867beb-45bf-4f06-92e0-7eeb8a3241e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHARGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.8781 - loss: 0.3700 - val_accuracy: 0.8933 - val_loss: 0.2840\n",
      "Epoch 2/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 39ms/step - accuracy: 0.9093 - loss: 0.2348 - val_accuracy: 0.8913 - val_loss: 0.2972\n",
      "Epoch 3/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 38ms/step - accuracy: 0.9354 - loss: 0.1616 - val_accuracy: 0.8840 - val_loss: 0.4208\n",
      "Epoch 4/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 39ms/step - accuracy: 0.9684 - loss: 0.0841 - val_accuracy: 0.8495 - val_loss: 0.5720\n",
      "Epoch 5/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 39ms/step - accuracy: 0.9843 - loss: 0.0439 - val_accuracy: 0.8708 - val_loss: 0.9552\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8690 - loss: 1.0048\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
      "CNN Model Accuracy: 0.8707804083824158\n",
      "CNN Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.94      0.93      4803\n",
      "     Class 1       0.50      0.37      0.42       707\n",
      "\n",
      "    accuracy                           0.87      5510\n",
      "   macro avg       0.70      0.66      0.67      5510\n",
      "weighted avg       0.86      0.87      0.86      5510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.csv\")\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=100, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=100, padding='post')\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Define CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(5))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(5))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_padded, y_test)\n",
    "cnn_y_pred = cnn_model.predict(X_test_padded)\n",
    "cnn_y_pred_classes = (cnn_y_pred > 0.5).astype(int).flatten()  # Convert probabilities to binary classes\n",
    "cnn_report = classification_report(y_test, cnn_y_pred_classes, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "print(\"CNN Model Accuracy:\", cnn_accuracy)\n",
    "print(\"CNN Model Classification Report:\\n\", cnn_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb79fd8-059c-4da8-9ec9-d8b6219dde5d",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70716120-c9ad-4046-9188-62ceaa2d8466",
   "metadata": {},
   "source": [
    "Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) architecture that addresses some of the shortcomings of traditional RNNs, such as the vanishing gradient problem.GRUs represent a powerful advancement in recurrent neural network architectures, offering improved performance and efficiency in sequence modeling tasks compared to traditional RNNs. Their ability to capture long-term dependencies while maintaining computational efficiency makes them a preferred choice for various NLP and time series prediction applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042e1cef-2708-4b1e-acc3-ddd198750cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHARGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 112ms/step - accuracy: 0.8722 - loss: 0.3825 - val_accuracy: 0.8717 - val_loss: 0.3836\n",
      "Epoch 2/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 111ms/step - accuracy: 0.8799 - loss: 0.3685 - val_accuracy: 0.8717 - val_loss: 0.3858\n",
      "Epoch 3/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 113ms/step - accuracy: 0.8783 - loss: 0.3721 - val_accuracy: 0.8717 - val_loss: 0.3847\n",
      "Epoch 4/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 112ms/step - accuracy: 0.8776 - loss: 0.3632 - val_accuracy: 0.8900 - val_loss: 0.3029\n",
      "Epoch 5/5\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 114ms/step - accuracy: 0.8973 - loss: 0.2623 - val_accuracy: 0.8924 - val_loss: 0.2889\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.8899 - loss: 0.2987\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step\n",
      "GRU Model Accuracy: 0.892377495765686\n",
      "GRU Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.96      0.94      4803\n",
      "     Class 1       0.63      0.40      0.49       707\n",
      "\n",
      "    accuracy                           0.89      5510\n",
      "   macro avg       0.77      0.68      0.71      5510\n",
      "weighted avg       0.88      0.89      0.88      5510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\BHARGAVI\\Downloads\\project_data\\ghc_test.csv\")\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 100\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Define GRU model\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_sequence_length))\n",
    "gru_model.add(GRU(units=128))  # GRU layer\n",
    "gru_model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "gru_model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate GRU model\n",
    "gru_loss, gru_accuracy = gru_model.evaluate(X_test_padded, y_test)\n",
    "gru_y_pred = gru_model.predict(X_test_padded)\n",
    "gru_y_pred_classes = (gru_y_pred > 0.5).astype(int).flatten()  # Convert probabilities to binary classes\n",
    "gru_report = classification_report(y_test, gru_y_pred_classes, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "print(\"GRU Model Accuracy:\", gru_accuracy)\n",
    "print(\"GRU Model Classification Report:\\n\", gru_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8b03b-9306-468a-8d84-b4e2bd005a09",
   "metadata": {},
   "source": [
    "The selected GRU (Gated Recurrent Unit) model has shown the highest accuracy among the compared models, achieving approximately 89.24%. It demonstrates good precision and recall metrics, particularly in distinguishing between the two classes (Class 0 and Class 1). The model's performance indicates its capability to effectively capture and utilize sequential dependencies in the data, making it well-suited for tasks such as sentiment analysis or sequence prediction. Compared to CNN and LSTM models, the GRU model performs better in terms of accuracy and metrics for Class 1, highlighting its strength in handling sequential data effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
